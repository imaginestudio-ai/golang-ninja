# OpenConfig

OpenConfig is a group of network operators (see the _Further reading_ section) with the common goal of streamlining the way we manage and operate networks. They welcome anyone operating a production network as a member and, more recently, have started to accept contributions from vendors when more than one of them implements the same feature (that they want to include in a YANG model).

Their initial focus was to create a set of vendor-neutral YANG data models based on common operational use cases and requirements from the field. This later expanded to include vendor-neutral **Remote Procedure Calls** (**RPCs**) for configuring, streaming telemetry, performing operational commands, and manipulating forwarding entries (see _Further reading_) on network devices. In this chapter, we will focus primarily on the OpenConfig RPCs, as we already covered YANG data models in [_Chapter 8_](https://subscription.imaginedevops.io/book/cloud-and-networking/9781800560925/2B16971_08.xhtml#_idTextAnchor182), _Network APIs_.

One thing that sets OpenConfig apart from other similar initiatives is that they not only work publicly on the specifications but also write open source code that implements these specifications, helping you to interact with OpenConfig-compliant devices. They write most of these projects in Go, including but not limited to ygot, gNxI Tools, the gNMI collector, the gNMI CLI utility, the gNMI test framework, gRPC tunnels, and IS-IS LSDB parsing (see _Further reading_). We encourage you to explore those projects, especially the ones we do not cover in this book, as they target a wide range of network-related applications.

At the time of writing, OpenConfig includes four gRPC services:

-   **gRPC Network Management Interface** (**gNMI**): For streaming telemetry and configuration management
-   **gRPC Network Operations Interface** (**gNOI**): For executing operational commands on network devices
-   **gRPC Routing Information Base Interface** (**gRIBI**): To let an external client inject routing entries on a network element
-   **gRPC Network Security Interface** (**gNSI**): Infrastructure services for securing access to a compliant network device

In the following sections, we will examine the following common operational tasks:

-   Device provisioning, with the gNMI `Set` RPC, to label correctly the primary and backup interfaces between two nodes in the lab topology
-   Streaming telemetry, with the `Subscribe` RPC, where a Go program reacts to a gNMI telemetry stream to make changes to the network
-   Network operations, with a `traceroute` example with the gNOI `Traceroute` RPC, to check that all the forwarding paths in the network are working as expected

Bookmark

# Technical requirements

You can find the code examples for this chapter in the book’s GitHub repository (see _Further reading_), in the `ch09` folder.

Important Note

We recommend you execute the Go programs in this chapter in a virtual lab environment. Refer to the appendix for prerequisites and instructions on how to build the fully configured network topology.

The first example we discuss in the following section explores gNMI to configure network devices with Go.

Bookmark

# Device provisioning

In [_Chapter 6_](https://subscription.imaginedevops.io/book/cloud-and-networking/9781800560925/2B16971_06.xhtml#_idTextAnchor144), _Configuration Management_, we discussed applying the desired configuration state on a network device. Network engineers routinely have to log in to network devices to provision new services, bring up new connections, or remove outdated configurations. We covered the different transport options available to configure network devices such as SSH or HTTP in the same chapter, and in [_Chapter 8_](https://subscription.imaginedevops.io/book/cloud-and-networking/9781800560925/2B16971_08.xhtml#_idTextAnchor182), _Network APIs_, we added gRPC as another option.

We briefly touched on modeling network device configurations with a data modeling language such as YANG, so we could move from configuring networks with semi-structured vendor-specific CLI syntax to a model where we exchange structured data with the network to change its configuration state.

OpenConfig defines a gRPC service specifically for configuration management called gNMI. It aims to provide a common gRPC protobuf definition that any vendor can implement, alongside their existing proprietary gRPC services.

The protobuf definition for gNMI is as follows:

```markup
service gNMI {
   rpc Capabilities(CapabilityRequest) returns (CapabilityResponse);
   rpc Get(GetRequest) returns (GetResponse);
   rpc Set(SetRequest) returns (SetResponse);
   rpc Subscribe(stream SubscribeRequest) returns (stream SubscribeResponse);
}
```

gNMI particularly offers configuration management capabilities via the `Set` RPC that you can use to make changes on a target node. The gNMI specification (see _Further reading_) has extensive documentation on all available gNMI RPCs. In this section, we will focus on `Set`.

## Set RPC

The `Set` RPC lets you change the state of a target network device. You do this by sending a `SetRequest` message that encodes all changes you want to make.

You can update, replace, or delete values in the data tree of the target device in a single transaction, using dedicated fields of the `SetRequest` message. This means that unless the target can apply every specified change, it must roll all of them back and return to its previous state. The following protobuf definition shows the options you have in a `SetRequest` message:

```markup
message SetRequest {
   Path prefix = 1;
   repeated Path delete = 2;
   repeated Update replace = 3;
   repeated Update update = 4;
   repeated gnmi_ext.Extension extension = 5;
}
```

The field called `Path` in `SetRequest` encodes a YANG data tree path. It’s worth noting that gNMI is not limited to using OpenConfig YANG models; it works equally well with vendor-defined YANG models. gNMI describes the data tree path as a series of `PathElem` (path elements). Each one of these is a data tree node that has a name, and it may have one or more attributes (keys) associated with it:

```markup
message Path {
  string origin = 2;
  repeated PathElem elem = 3;
  string target = 4;
}
message PathElem {
  string name = 1;
  map<string, string> key = 2;
}
```

For instance, the `/interfaces/interface[name=Ethernet2]/config/description` path lets you set the description on the `Ethernet2` interface on a target device. The only data node in this case that has an attribute is `interface`, which needs a `name`. To configure an IPv4 address on the native VLAN in that same interface, you can use a path that looks like this: `/interfaces/interface[name=Ethernet2]/subinterfaces/subinterface[index=0]/ipv4/addresses/address[ip=192.0.2.2]`. In this case, you need to add the `subinterface` index, as the interface could have IP addresses on different sub-interfaces.

Once you have identified the data path, you need to build the content that has the new values you want to set on the target device, which is a data instance of a YANG schema. You only need this for `replace` and `update`. For `delete`, the path is enough to tell the target device what to remove from the configuration.

An `Update` message that you would use to send the values for either `replace` or `update` has a `Path` and `TypedValue` pair. The latter lets you encode the content in different formats:

```markup
message Update {
  Path path = 1;
  TypedValue val = 3;
  uint32 duplicates = 4;
}
message TypedValue {
  oneof value {
    string string_val = 1;
    int64 int_val = 2;
    uint64 uint_val = 3;
    bool bool_val = 4;
    bytes bytes_val = 5;
    double double_val = 14;
    ScalarArray leaflist_val = 8;
    google.protobuf.Any any_val = 9;
    bytes json_val = 10;
    bytes json_ietf_val = 11;
    string ascii_val = 12;
    bytes proto_bytes = 13;
  }
}
```

A value could be a string for an interface description, such as `PRIMARY: TO -> CVX:swp1` or a JSON value to describe the IPv4 address of an interface such as `{"config":{"ip":"192.0.2.2","prefix-length":31}}`.

## Using gNMI to configure network interfaces

The virtual lab topology for this chapter, which you can bring up by running `make lab-full` from the root of this book’s GitHub repository, has two connections between `ceos` and `cvx`. They have IPv4 addresses configured already, but they don’t have a description that lets you identify the roles of these interfaces, whether they are the primary or the backup link:

![Figure 9.1 – A dual link between ceos and cvx](https://static.packt-cdn.com/products/9781800560925/graphics/image/B16971_09_01.jpg)

Figure 9.1 – A dual link between ceos and cvx

In the next example, we add a description to those interfaces on the `ceos` side via gNMI. To do this, we use the gNMIc package (`karimra/gnmic/api`). We chose gNMIc over the official gNMI package (`openconfig/gnmi`) because it’s more developer-friendly and higher-level. It lets us conveniently encode the gNMI paths as strings, instead of Go data structures, as the gNMIc docs (see _Further reading_) describe. You can find the code for this example in the `ch09/gnmi` directory of this book’s GitHub repository (see _Further reading_).

The gNMIc package has a `NewTarget` function that creates a new gNMI target device. In the following example, we wrap this function in the `createTarget` method:

```markup
func (r Router) createTarget() (*target.Target, error) {
      return api.NewTarget(
           api.Name("gnmi"),
           api.Address(r.Hostname+":"+r.Port),
           api.Username(r.Username),
           api.Password(r.Password),
           api.Insecure(r.Insecure),
      )
}
```

The first step in the code is to read the connection details from a YAML file (`input.yml`) to create this target device:

```markup
# input.yml
- hostname: clab-netgo-ceos
  port: 6030
  insecure: true
  username: admin
  password: admin
```

We store all target devices in the `Routers` data structure. In our case, we only have one device (`clab-netgo-ceos`) but the connection details are a list, so we could’ve added more devices if we wanted to. Now, with the target data, we use the `CreateGNMIClient` method to set up the underlying gRPC connection to the target device (`clab-netgo-ceos:6030`):

```markup
func main() {
  /* ... <omitted for brevity > ... */
  for _, router := range inv.Routers {
    tg, err := router.createTarget()
    // process error
 
    ctx, cancel := context.WithCancel(
    context.Background())
    defer cancel()
 
    err = tg.CreateGNMIClient(ctx)
    // process error
    defer tg.Close()
  /* ... <continues next > ... */
}
```

With the connection established, we now can send the `Set` requests. Another YAML file (`api-ceos.yml`) has a list of parameters for each request: `prefix`, `encoding`, `path`, and `value`. You can add `prefix` when you want to reduce the length of a path. In our Go program, we save this list of parameters in the `info` slice:

```markup
# api-ceos.yml
- prefix: "/interfaces/interface[name=Ethernet2]"
  encoding: "json_ietf"
  path: '/subinterfaces/subinterface[index=0]/ipv4/addresses/address[ip=192.0.2.2]'
  value: '{"config":{"ip":"192.0.2.2","prefix-length":31}}'
- prefix: ""
  encoding: "json_ietf"
  path: '/interfaces/interface[name=Ethernet2]/config/description'
  value: 'PRIMARY: TO -> CVX:swp1''
## ... <omitted for brevity > ... ##
```

The last step is to iterate over the `info` slice, build a `Set` request with the `NewSetRequest` function, and send it to the target device using the `Set` method:

```markup
func main() {
  /* ... <continues from before > ... */
    for _, data := range info {
      setReq, err := api.NewSetRequest(
              api.Update(
                    api.Path(data.Prefix+data.Path),
                    api.Value(data.Value, data.Encoding)),
      )
      // process error
  
      configResp, err := tg.Set(ctx, setReq)
      // process error
      fmt.Println(prototext.Format(configResp))
    }
  }
}
```

Here, `NewSetRequest` has only one `Update` message, but you could include several messages in a single request.

You get the following output when running this example:

```markup
ch09/gnmi$ go run main.go 
response: {
  path: {
    elem: {
      name: "interfaces"
    }
    elem: {
      name: "interface"
      key: {
        key: "name"
        value: "Ethernet2"
      }
    }
    elem: {
      name: "subinterfaces"
    }
    elem: {
      name: "subinterface"
      key: {
        key: "index"
        value: "0"
      }
    }
    elem: {
      name: "ipv4"
    }
    elem: {
      name: "addresses"
    }
    elem: {
      name: "address"
      key: {
        key: "ip"
        value: "192.0.2.2"
      }
    }
  }
  op: UPDATE
}
timestamp: 1660148355191641746
response: {
  path: {
    elem: {
      name: "interfaces"
    }
    elem: {
      name: "interface"
      key: {
        key: "name"
        value: "Ethernet2"
      }
    }
    elem: {
      name: "config"
    }
    elem: {
      name: "description"
    }
  }
  op: UPDATE
}
timestamp: 1660148355192866023
## ... <omitted for brevity > ... ##
```

What you see on the terminal screen are the `SetResponse` messages, containing the `path`, `response`, and `timestamp` values of the operation:

```markup
message SetResponse {
  Path prefix = 1;
  repeated UpdateResult response = 2;
  int64 timestamp = 4;
  repeated gnmi_ext.Extension extension = 5;
}
```

If you connect to the `ceos` device now, you will see the following in its running configuration:

```markup
interface Ethernet2
   description PRIMARY: TO -> CVX:swp1
   no switchport
   ip address 192.0.2.2/31
!
interface Ethernet3
   description BACKUP: TO -> CVX:swp2
   no switchport
   ip address 192.0.2.4/31
!
```

Configuring network devices is one of those repetitive tasks that most network engineers spend a good amount of time on, so automating this process has the potential to have a good return on investment.

The years of work of the OpenConfig working group, which released the official gNMI package (`openconfig/gnmi`), set the path for the emergence of other open source packages and libraries such as gNMIc (`karimra/gnmic`) and pyGNMI (`akarneliuk/pygnmi`), creating a community around these vendor-neutral gRPC services to drive consistent automation practices in our networks.

In the following section, we will cover another OpenConfig gRPC service that enhances your network visibility capabilities.

Bookmark

# Streaming telemetry

Traditionally, network engineers have relied on the **Simple Network Management Protocol** (**SNMP**) to gather state information from network devices. Devices encode this information in a binary format using the **Abstract Syntax Notation One** (**ASN.1**) and send it to a receiver, typically a collector or a **Network Management System** (**NMS**). The latter would use one of the **Management Information Bases** (**MIBs**) to decode the received information and store it locally for further processing.

This has been the way we’ve done network monitoring for decades, but this approach has room for improvement:

-   The limited number of vendor-neutral data models means that even the basic things require unique MIBs that you may need to update every time you do a major network OS upgrade.
-   MIBs use a notation defined by a subset of ASN.1, which isn’t the best way to structure values. It has no concept of lists or key-value pairs. Instead, you must implement these with indexed values and extra lookup tables.
-   SNMP uses UDP as its transport protocol to avoid putting an extra burden on the collector. This means that you could miss some events completely, leaving blind spots in the stream of telemetry data.
-   Since SNMP primarily relies on polling, we can only see aggregated values and may miss important state transitions.
-   SNMP does not generally timestamp when a value changes. Collectors can only infer timing based on the time of collection.

gNMI offers a new approach to network monitoring via a dedicated `Subscribe` RPC. At the very least, it offers the same capabilities as SNMP but takes it further, making the protocol more feature-rich and versatile:

-   One of the greatest improvements is telemetry streaming. Now, you can continuously receive any value of the operational YANG tree from a network device, which gives you better visibility into all state transitions along with their timestamps.
-   You have a choice to receive telemetry data only when there is a change as opposed to a periodic transmission.
-   Thanks to the underlying gRPC transport, gNMI supports both dial-in and dial-out connection methods and delivers messages using a reliable HTTP/2 protocol.
-   OpenConfig defines vendor-neutral YANG models to describe the operational state of a network device, which enables clients to parse and process the received data from different vendors in a standard pipeline.

Important Note

Even with streaming telemetry, you are not necessarily getting an update for every counter increment. Network devices have local processes that periodically poll internal data stores to get the latest metrics or stats, such as interface packet counters, which they feed to their gNMI process. Hence, how real-time the data you receive is depends not only on how often you get streaming messages but also on the internal polling cadence. Still, you will probably see the most relevant system events, such as BGP state transitions, which you would otherwise miss with SNMP.

These features are just a subset of the gNMI capabilities. The gNMI specification (see _Further reading_) can serve as a good reference for all gNMI protocol features. Next, we examine the gNMI protobuf message for the telemetry service to help you understand how it works.

## Subscribe RPC

gNMI defines a single RPC to subscribe to a telemetry stream. Network devices receive one or more `SubscribeRequest` messages and respond with a stream of `SubscribeResponse` messages:

```markup
service gNMI {
     rpc Subscribe(stream SubscribeRequest) returns (stream SubscribeResponse);
}
```

gNMI clients have different options to control their telemetry subscriptions. The following figure shows the composition of the `SubscribeRequest` message, highlighting some of these options:

![Figure 9.2 – gNMI subscribe protobuf messages](https://static.packt-cdn.com/products/9781800560925/graphics/image/B16971_09_02.jpg)

Figure 9.2 – gNMI subscribe protobuf messages

The most basic way to control the telemetry subscription is by specifying `Path` and `SubscriptionMode`:

-   **Path**: References the part of the YANG tree you want to monitor. You can subscribe to anything, from the entire device state to just a single leaf value. It follows the gNMI path convention (see _Further reading_).
-   **SubscriptionMode**: Determines whether to send the telemetry on-change or periodically:
    
    ```markup
    enum SubscriptionMode {
    ```
    
    ```markup
         TARGET_DEFINED = 0;
    ```
    
    ```markup
         ON_CHANGE      = 1;
    ```
    
    ```markup
         SAMPLE         = 2;
    ```
    
    ```markup
    }
    ```
    

In return, a network device sends you a stream of response messages with the following information:

-   **TypedValue**: The most critical field, containing the actual telemetry value
-   **Path**: The full gNMI path of the value, which identifies the unique YANG leaf node
-   **timestamp**: To help you arrange and process received data in the right order or find out when a value last changed for those that do not change frequently:
    
    ```markup
    message Notification {
    ```
    
    ```markup
         int64 timestamp = 1;
    ```
    
    ```markup
         Path prefix = 2;
    ```
    
    ```markup
         string alias = 3;
    ```
    
    ```markup
         repeated Update update = 4;
    ```
    
    ```markup
         repeated Path delete = 5;
    ```
    
    ```markup
         bool atomic = 6;
    ```
    
    ```markup
    }
    ```
    
    ```markup
    message Update {
    ```
    
    ```markup
         Path path = 1;
    ```
    
    ```markup
         TypedValue val = 3;
    ```
    
    ```markup
         uint32 duplicates = 4;
    ```
    
    ```markup
    }
    ```
    

We are just scratching the surface of the `Subscribe` RPC. You can check the `gnmi.proto` file to see the complete set of protobuf messages and read the telemetry section of the gNMI specification (see _Further reading_) to get a better idea of the capabilities and features offered by the protocol. Here are some features you can learn about that we don’t cover in this book:

-   gNMI lets you poll or take an instant one-off (`ONCE`) snapshot of telemetry values.
-   Some network devices can send several `Update` messages bundled in a single `SubscribeResponse`. This comes at the expense of reduced timestamp accuracy, since there’s only a single timestamp for all transported values.
-   If you are not interested in seeing every single value, you can let a network device aggregate those values.
-   For values that different YANG models define, you can specify the definition you prefer to use.

Important Note

As with OpenConfig YANG models, the exact set of implemented features varies from vendor to vendor.

## Streaming telemetry processing pipelines with gNMI

To receive or collect the data from a gNMI-compliant network device, you could use the Go gNMI client implementation from the official gNMI repository (see _Further reading_). Another alternative is gNMIc (see _Further reading_), which builds on top of the official gNMI client and provides more capabilities, such as data transformation and wide support of northbound interfaces.

gNMIc can serve as a link between a network device and a **T****ime-Series Database** (**TSDB**) or a message queue, as it can transform the received telemetry data into a format popular open source projects, such as Prometheus, InfluxDB, NATS, and Kafka, can understand. You can run gNMIc as a command-line tool to interact with network devices or as a daemon, subscribing to telemetry data and publishing it into a message queue or a database.

## Event-manager sample program

Let’s examine one example of a telemetry processing pipeline via an implementation of a primitive event-manager application. The goal of this program is to react to an increased packet rate by temporarily enabling a backup interface to redistribute incoming traffic. The following diagram depicts the high-level architecture of the telemetry processing pipeline and includes the following main components:

-   A gNMIC process running as a daemon, collecting and processing network telemetry data
-   A TSDB (Prometheus) storing the collected telemetry data
-   AlertManager (see _Further reading_) processing alerts received from Prometheus and triggering external events
-   A Go program that implements the event-manager business logic:

![Figure 9.3 – The event-manager application](https://static.packt-cdn.com/products/9781800560925/graphics/image/B16971_09_03.jpg)

Figure 9.3 – The event-manager application

You can spin up these components with `make gnmic-start` from the root of this book’s GitHub repository (see _Further reading_). This command starts the gNMIc daemon and brings up Prometheus, Grafana, and AlertManager using `docker-compose`. These applications now run alongside our test lab topology and interact with it over standard network interfaces:

![Figure 9.4 – The event-manager topology](https://static.packt-cdn.com/products/9781800560925/graphics/image/B16971_09_04.jpg)

Figure 9.4 – The event-manager topology

We configured these applications using a series of files located in the `topo`\-`full/workdir/` (see _Further reading_) directory of this book’s GitHub repository (see _Further reading_). These files get mounted into their respective containers, as we define in the configuration files of either Containerlab (`topo.yml` – see _Further reading_) or Docker Compose (`docker-compose.yml` – see _Further reading_). Here’s a brief description of the role these applications play in our setup:

-   The gNMIc daemon process runs in `Host-3` of the test topology. It subscribes to telemetry data from the `cvx` device and exposes it as Prometheus-style metrics. We manage these settings in the `gnmic.yaml` file that looks like this:
    
    ```markup
    targets:
    ```
    
    ```markup
     "clab-netgo-cvx:9339":
    ```
    
    ```markup
        username: cumulus
    ```
    
    ```markup
        password: cumulus
    ```
    
    ```markup
    subscriptions:
    ```
    
    ```markup
      counters:
    ```
    
    ```markup
        target: netq
    ```
    
    ```markup
        paths:
    ```
    
    ```markup
          - /interfaces
    ```
    
    ```markup
        updates-only: true
    ```
    
    ```markup
    outputs:
    ```
    
    ```markup
      prom-output:
    ```
    
    ```markup
        type: prometheus
    ```
    
    ```markup
        listen: ":9313"
    ```
    
-   You can find the Prometheus configuration values in the `prometheus.yml` file. We configure it to scrape the gNMIc endpoint every 2 seconds and store the collected data in its TSDB:
    
    ```markup
    scrape_configs:
    ```
    
    ```markup
      - job_name: 'event-trigger'
    ```
    
    ```markup
        scrape_interval: 2s
    ```
    
    ```markup
        static_configs:
    ```
    
    ```markup
          - targets: ['clab-netgo-host-3:9313']
    ```
    
-   The same configuration file includes a reference to the alert definition file, called `alert.rules`, and the connection details of the AlertManager:
    
    ```markup
    rule_files:
    ```
    
    ```markup
      - 'alert.rules'
    ```
    
    ```markup
    alerting:
    ```
    
    ```markup
      alertmanagers:
    ```
    
    ```markup
      - scheme: http
    ```
    
    ```markup
        static_configs:
    ```
    
    ```markup
        - targets:
    ```
    
    ```markup
          - "alertmanager:9093"
    ```
    
-   Inside of the `alert.rules` file, we define a single alert we call `HighLinkUtilization`. Every 10 seconds, Prometheus checks whether the incoming packet rate has exceeded a predefined threshold of 50 packets per 30-second interval, in which case it fires an alert and sends it to the AlertManager:
    
    ```markup
    groups:
    ```
    
    ```markup
    - name: thebook
    ```
    
    ```markup
      interval: 10s
    ```
    
    ```markup
      rules:
    ```
    
    ```markup
      - alert: HighLinkUtilization
    ```
    
    ```markup
        expr: rate(interfaces_interface_state_counters_in_pkts[30s]) > 50
    ```
    
    ```markup
        for: 0m
    ```
    
    ```markup
        labels:
    ```
    
    ```markup
          severity: warning
    ```
    
    ```markup
        annotations:
    ```
    
    ```markup
          summary: Transit link {{ $labels.interface_name }} is under high load
    ```
    
    ```markup
          description: "Transit link {{ $labels.interface_name }} is under high load LABELS = {{ $labels }}"
    ```
    
    ```markup
          value: '{{ $value }}'
    ```
    
-   AlertManager has its own configuration file, called `alertmanager.yml`, that controls how to aggregate and route incoming alerts from Prometheus. In our case, we have a single alert type, so we only need one route. We decrease the default aggregation timers to enable faster reaction time and specify the webhook URL where to send these alerts:
    
    ```markup
    route:
    ```
    
    ```markup
      receiver: 'event-manager'
    ```
    
    ```markup
      group_wait: 5s
    ```
    
    ```markup
      group_interval: 10s
    ```
    
    ```markup
    receivers:
    ```
    
    ```markup
      - name: 'event-manager'
    ```
    
    ```markup
        webhook_configs:
    ```
    
    ```markup
        - url: http://clab-netgo-host-2:10000/alert
    ```
    
-   event-manager parses the alert and toggles a backup interface to re-balance the traffic coming into the `cvx` device. Its behavior is fairly static, so we don’t need a configuration file for it.

The event-manager program implements a standard web server that listens to incoming requests and dispatches them to a handler function. Here, we decode the received Prometheus alert and invoke the `toggleBackup` function based on its status:

```markup
func alertHandler(w http.ResponseWriter, req *http.Request) {
  log.Println("Incoming alert")
  var alerts Alerts
  err := json.NewDecoder(req.Body).Decode(&alerts)
  // process error
 
  for _, alert := range alerts.Alerts {
    if alert.Status == "firing" {
      if err := toggleBackup(alert.Labels.InterfaceName, "permit"); err != nil {
        w.WriteHeader(http.StatusInternalServerError)
        return
      }
      continue
    }
 
    if err := toggleBackup(alert.Labels.InterfaceName, "deny"); err != nil {
      w.WriteHeader(http.StatusInternalServerError)
      return
    }
  }
  w.WriteHeader(http.StatusOK)
}
```

We have two _uplinks_ between the `cvx` and `ceos` devices, and we only use one of them by default. The backup uplink does BGP ASN prepending and only receives traffic when we announce more specific or disaggregated prefixes. The `toggleBackup` function does this by toggling a permit/deny statement on an IP prefix list (on `cvx`), thereby enabling or disabling the BGP disaggregation behavior:

```markup
var (
  backupRules = map[string][]int{
    "swp1": {10, 20},
  }
)
 
func toggleBackup(intf string, action string) error {
  log.Printf("%s needs to %s backup prefixes",
              intf, action)
  ruleIDs, ok := backupRules[intf]
  // process error
 
  var pl PrefixList
  pl.Rules = make(map[string]Rule)
  for _, ruleID := range ruleIDs {
    pl.Rules[strconv.Itoa(ruleID)] = Rule{
      Action: action,
    }
  }
 
 
  var payload nvue
  payload.Router.Policy.PrefixLists = map[string]PrefixList{
    plName: pl,
  }
 
  b, err := json.Marshal(payload)
  // process error
 
  return sendBytes(b)
}
```

The final `sendBytes` function applies the constructed configuration using the three-stage commit process we discussed in [_Chapter 6_](https://subscription.imaginedevops.io/book/cloud-and-networking/9781800560925/2B16971_06.xhtml#_idTextAnchor144), _Configuration Management_.

## Visualizing the data

You can connect to the local instance of Grafana running at `:3000` using `admin` as the username/password to test the complete telemetry-driven pipeline in action. This Grafana instance comes up pre-integrated with Prometheus as its data source, and it includes a pre-built `event-manager` dashboard that plots the incoming packet rate for both `cvx` links to `ceos`.

Run `make traffic-start` from the root of this book’s GitHub repository (see _Further reading_) to generate traffic in the lab topology. All traffic should initially flow over the primary connection between `cvx` and `ceos` (`swp1`).

Next, we want to start the event-manager application so that we can load-balance traffic across both connections. To do this, run the event-manager Go application inside the `host-2` container. This translates to the command that we execute in the following snippet:

```markup
$ sudo ip netns exec clab-netgo-host-2 /usr/local/go/bin/go run ch09/event-manager/main.go
AlertManager event-triggered webhook
2022/08/01 21:51:13 Starting web server at 0.0.0.0:10000
```

Open a new terminal window or tab and run `make traffic-start` again, but increase the traffic generation period from the default `60s` using the `DURATION` variable. For example, the following command would generate traffic for 2 minutes:

```markup
$ DURATION=2m make traffic-start
```

This can help you see the longer-term effect of traffic re-balancing. Logs should show that the traffic rate has triggered an alert and the application implemented corrective actions:

```markup
$ sudo ip netns exec clab-netgo-host-2 /usr/local/go/bin/go run ch09/event-manager/main.go
AlertManager event-triggered webhook
2022/08/01 21:51:13 Starting web server at 0.0.0.0:10000
ch09/event-manager/main.go
2022/08/01 21:53:10 Incoming alert
2022/08/01 21:53:10 swp1 needs to permit backup prefixes
2022/08/01 21:53:10 Created revisionID: changeset/cumulus/2022-08-01_21.53.10_ASP0
{
  "state": "apply",
  "transition": {
    "issue": {},
    "progress": ""
  }
}
2022/08/01 21:54:00 Incoming alert
2022/08/01 21:54:00 swp1 needs to deny backup prefixes
2022/08/01 21:54:00 Created revisionID: changeset/cumulus/2022-08-01_21.54.00_ASP2
{
  "state": "apply",
  "transition": {
    "issue": {},
    "progress": ""
  }
}
2022/08/01 21:54:00 swp2 needs to permit backup prefixes
2022/08/01 21:54:00 Could not find a backup prefix for swp2
2022/08/01 21:54:20 Incoming alert
2022/08/01 21:54:20 swp2 needs to deny backup prefixes
2022/08/01 21:54:20 Could not find a backup prefix for swp2
2022/08/01 21:54:30 Incoming alert
2022/08/01 21:54:30 swp1 needs to permit backup prefixes
2022/08/01 21:54:30 Created revisionID: changeset/cumulus/2022-08-01_21.54.30_ASP4
{
  "state": "apply",
  "transition": {
    "issue": {},
    "progress": ""
  }
}
2022/08/01 21:55:20 Incoming alert
2022/08/01 21:55:20 swp1 needs to deny backup prefixes
2022/08/01 21:55:20 Created revisionID: changeset/cumulus/2022-08-01_21.55.20_ASP6
{
  "state": "apply",
  "transition": {
    "issue": {},
    "progress": ""
  }
}
```

All three of the tests we performed should get you a similar-looking graph:

![Figure 9.5 – Event-manager visualization](https://static.packt-cdn.com/products/9781800560925/graphics/image/B16971_09_05.jpg)

Figure 9.5 – Event-manager visualization

Streaming telemetry is a powerful capability that you can adapt to a wide variety of business use cases. However, most of these use cases are specific to the operating network environment, so it’s hard to come up with a set of _killer applications_ that would apply to every network. Hence, it’s important to know how to implement the required business logic in code, which is what we have tried to show you in this chapter.

In the following section, we cover another OpenConfig gRPC service you can use to automate operational tasks.

Bookmark

# Network operations

In the preceding sections, we explored how the OpenConfig management interface approaches two common network automation use cases: configuration management and operational state collection. These two tasks alone can get you a long way in your network automation journey, but there is a set of common operational tasks that don’t fall into either of these categories.

To automate all aspects of network operations, we need to perform tasks such as network device reloads, software life cycle management, and counter and adjacency resets. You normally execute these activities as part of interactive CLI workflows, with prompts and warnings that assume a human operator is involved in the process. This makes the automation of these tasks a major undertaking, as we have to resort to screen-scraping, which increases the already high risk of these tasks.

To address these challenges, OpenConfig proposed a new gRPC API, designed to abstract away the interactive commands and surface these network operations capabilities in a standard, vendor-neutral way.

### gNOI

gNOI defines a list of gRPC services that address a wide range of network operations use cases. Each service represents one operational process with a set of actions, and the following table includes a few examples to give you an idea of the challenges gNOI attempts to solve:

<table id="table001-3" class="No-Table-Style _idGenTablePara-1"><colgroup><col> <col> <col></colgroup><tbody><tr class="No-Table-Style"><td class="No-Table-Style"><p><span class="No-Break"><strong class="bold">Service</strong></span></p></td><td class="No-Table-Style"><p><span class="No-Break"><strong class="bold">Description</strong></span></p></td><td class="No-Table-Style"><p><span class="No-Break"><strong class="bold">RPC examples</strong></span></p></td></tr><tr class="No-Table-Style"><td class="No-Table-Style"><p><span class="No-Break">OS</span></p></td><td class="No-Table-Style"><p>NOS <span class="No-Break">package management</span></p></td><td class="No-Table-Style"><p>Install, Activate, <span class="No-Break">and Verify</span></p></td></tr><tr class="No-Table-Style"><td class="No-Table-Style"><p><span class="No-Break">File</span></p></td><td class="No-Table-Style"><p><span class="No-Break">File operations</span></p></td><td class="No-Table-Style"><p>Get, Transfer, Put, <span class="No-Break">and Remove</span></p></td></tr><tr class="No-Table-Style"><td class="No-Table-Style"><p><span class="No-Break">L2</span></p></td><td class="No-Table-Style"><p>L2 <span class="No-Break">protocols operations</span></p></td><td class="No-Table-Style"><p>ClearNeighborDiscovery <span class="No-Break">and ClearLLDPInterface</span></p></td></tr><tr class="No-Table-Style"><td class="No-Table-Style"><p><span class="No-Break">Cert</span></p></td><td class="No-Table-Style"><p><span class="No-Break">Certificate management</span></p></td><td class="No-Table-Style"><p>Rotate, Install, GenerateCSR, <span class="No-Break">and RevokeCertificates</span></p></td></tr><tr class="No-Table-Style"><td class="No-Table-Style"><p><span class="No-Break">System</span></p></td><td class="No-Table-Style"><p><span class="No-Break">System operations</span></p></td><td class="No-Table-Style"><p>Ping, Traceroute, Reboot, <span class="No-Break">and Time</span></p></td></tr></tbody></table>

Table 9.1 – gNOI use case examples

Some RPCs are a one-shot with immediate response, some stream responses synchronously until complete or canceled, and some work asynchronously.

The gNOI GitHub repository (see _Further reading_) protobuf files have the most recent list of actions for each service. At the time of writing, this is the top-level definition of the `system.proto` file (see _Further reading_):

```markup
service System {
     rpc Ping(PingRequest) returns (stream PingResponse) {}
     rpc Traceroute(TracerouteRequest) returns (stream TracerouteResponse) {}
     rpc Time(TimeRequest) returns (TimeResponse) {}
     rpc SetPackage(stream SetPackageRequest) returns (SetPackageResponse) {}
     rpc SwitchControlProcessor(SwitchControlProcessorRequest)
       returns (SwitchControlProcessorResponse) {}
     rpc Reboot(RebootRequest) returns (RebootResponse) {}
     rpc RebootStatus(RebootStatusRequest) returns (RebootStatusResponse) {}
     rpc CancelReboot(CancelRebootRequest) returns (CancelRebootResponse) {}
     rpc KillProcess(KillProcessRequest) returns (KillProcessResponse) {}
}
```

We don’t cover all gNOI RPCs in this book. Instead, we focus on just one and include an example program built around it.

## Traceroute RPC

Most, if not all, network engineers are familiar with the `traceroute` command. This is a common way to explore the forwarding path between a pair of network endpoints. When you run `traceroute` from a network device’s interactive shell, the terminal prints the result on your screen. With gNOI, `traceroute` is an action we request via an RPC with a `TracerouteRequest` message in the payload, and the result is a stream (one or many) of `TracerouteResponse` messages:

```markup
service System {
     rpc Traceroute(TracerouteRequest) returns (stream TracerouteResponse) {}
```

As with the `traceroute` command-line arguments and flags, the request message lets you specify options such as source address, the maximum number of hops, and whether to perform reverse DNS lookups:

```markup
message TracerouteRequest {
     string source = 1;      // Source addr to ping from.
     String destination = 2; // Destination addr to ping.
     Uint32 initial_ttl = 3; // Initial TTL. (default=1)
     int32 max_ttl = 4;      // Maximum number of hops. 
     Int64 wait = 5;         // Response wait-time (ns).
     Bool do_not_fragment = 6;  
     bool do_not_resolve = 7;
     /* ... <omitted for brevity > ... */
}
```

Each response message includes the results of a single measurement cycle, including the hop count, the round-trip time, and the responding address extracted from a probe reply:

```markup
message TracerouteResponse {
     /* ... <omitted for brevity > ... */
     int32 hop = 5;          // Hop number. required.
     string address = 6;     // Address of responding hop. 
     string name = 7;        // Name of responding hop.
     int64 rtt = 8;          // Round trip time in nanoseconds.
     /* ... <omitted for brevity > ... */
}
```

Now, let’s see an example of how to use the gNOI interface with Go.

## Path verifier application

In the streaming telemetry section of this chapter, we explored the implementation of an event-manager application that enables or disables a backup link as the traffic through the primary interface crosses a pre-defined threshold. We used Grafana to plot the traffic rate for both interfaces to confirm that the application works as intended.

In real-world automation use cases involving complex workflows, relying on visual clues is not always the right approach. Ideally, we need a programmatic way to verify that the backup link is actually working. We use the gNOI `Traceroute` RPC to check this in the next code example. The goal is to explore diverse network paths and confirm that we are forwarding some traffic flows over the backup interface. You can find the code example for this section in the `ch09/gnoi-trace` directory of this book’s GitHub repository (see _Further reading_).

We start by setting up a gRPC session to the `ceos` virtual network device and creating a new API client for the gNOI `System` service:

```markup
var target = "clab-netgo-ceos:6030"
import (
     "google.golang.org/grpc"
     "github.com/openconfig/gnoi/system"
)
func main() {
     conn, err := grpc.Dial(target, grpc.WithInsecure())
     // process error
     defer conn.Close()
     sysSvc := system.NewSystemClient(conn)
     ctx, cancel := context.WithCancel(context.Background())
     defer cancel()
  /* ... <continues next > ... */
}
```

Next, we create a `sync.WaitGroup` to coordinate all goroutines running traceroutes to different destinations. These goroutines send the collected results back to the `main` goroutine over the `traceCh` channel. For each traceroute destination encoded as `string`, the traceroute result includes a list of responded IP addresses per network hop.

To make it easier to compare lists of IP addresses in the following steps, we store them as a set using the `deckarep/golang-set` (`mapset`) third-party package, because Go doesn’t implement sets natively in the standard library. We encode the hop count implicitly as the index of the `[]``mapset.Set` array:

```markup
var destinations = []string{
           "203.0.113.251",
           "203.0.113.252",
           "203.0.113.253",
}
func main() {
     /* ... <continues from before > ... */
     var wg sync.WaitGroup
     wg.Add(len(destinations))
     traceCh := make(chan map[string][]mapset.Set,
                            len(destinations))
  /* ... <continues next > ... */
}
```

Each goroutine runs a single traceroute, and we only specify the source and destination fields of the `TracerouteRequest` message, leaving the rest options as default. As we receive responses, we store the results in the `route` slice. When the traceroute stops, which is when the error type is `io.EOF`, we send the accumulated response over the `traceCh` channel and call `wg.Done`:

```markup
var source = "203.0.113.3"
 
func main() {
  /* ... <continues from before > ... */
  for _, dest := range destinations {
    go func(d string) {
      defer wg.Done()
      retryMax := 3
      retryCount := 0
 
    START:
      response, err := sysSvc.Traceroute(ctx,
                        &system.TracerouteRequest{
                                 Destination: d,
                                 Source: source,
      })
      // process error
 
 
      var route []mapset.Set
      for {
        resp, err := response.Recv()
        if errors.Is(err, io.EOF) {
        // end of stream, traceroute completed
          break
        }
        // process error
 
        // timed out, restarting the traceroute
        if int(resp.Hop) > len(route)+1 {
          if retryCount > retryMax-1 {
            goto FINISH
          }
          retryCount += 1
          goto START
        }
 
        // first response
        if len(route) < int(resp.Hop) {
          route = append(route, mapset.NewSet())
        }
 
        // subsequent responses
          route[resp.Hop-1].Add(resp.Address)
        }
 
    FINISH:
      traceCh <- map[string][]mapset.Set{
               d: route,
             }
    }(dest)
  }
  wg.Wait()
  close(traceCh)
  /* ... <continues next > ... */
}
```

Since network devices have default control plane security settings that may restrict them from processing every incoming **ICMP** packet they receive, you might see gaps in your traceroute results. To overcome this, we use Go’s labels and `goto` statements in the code to retry a traceroute in case we don’t get any information for any one hop. `START` and `FINISH` are the two labels we used to implement this retry logic, with the latter serving as a fall-through case when we don’t get a result after several attempts.

Once we have completed all traceroute requests, we can process and analyze the results. To simplify the code logic, we first transform the data to store a map between a hop count and a set of IP addresses per traceroute destination:

```markup
func main() {
  /* ... <continues from before > ... */
  routes := make(map[int]map[string]mapset.Set)
 
  for trace := range traceCh {
    for dest, paths := range trace {
      for hop, path := range paths {
        if _, ok := routes[hop]; !ok {
          routes[hop] = make(map[string]mapset.Set)
        }
        routes[hop][dest] = path
      }
    }
  }
  /* ... <continues next > ... */
}
```

Finally, we can traverse over each hop and check whether there is a discrepancy between a set of responding IP addresses for different traceroute destinations, which would mean that the packets went over different paths. If we detect this, we print it on the screen:

```markup
func main() {
  /* ... <continues from before > ... */
  for hop, route := range routes {
    if hop == len(routes)-1 {
      continue
    }
    found := make(map[string]string)
    for myDest, myPaths := range route {
      for otherDest, otherPaths := range route {
        if myDest == otherDest {
          continue
        }
        diff := myPaths.Difference(otherPaths)
        if diff.Cardinality() == 0 {
          continue
        }
 
        v, ok := found[myDest]
        if ok && v == otherDest {
          continue
        }
 
        log.Printf("Found different paths at hop %d", hop)
        log.Printf("Destination %s: %+v", myDest, myPaths)
        log.Printf(
                "Destination %s: %+v",
                        otherDest,
                        otherPaths,
                        )
        found[otherDest] = myDest
      }
    }
  }
  log.Println("Check complete")
}
```

You can run this program from the `ch09/gnoi-trace` folder. Make sure `lab-full` is up and running first. You should see output like the following:

```markup
ch09/gnoi-trace$ go run main.go
2022/06/26 16:51:10 Checking if routes have different paths
2022/06/26 16:51:16 Missed at least one hop in 203.0.113.251
2022/06/26 16:51:16 retrying 203.0.113.251
2022/06/26 16:51:17 Check complete
```

Generate traffic with `make traffic-start` and run this program again. In another tab, run simultaneously the event-manager application from the `clab-netgo-host-2` host to activate the backup link:

```markup
$ DURATION=2m make traffic-start
docker exec -d clab-netgo-cvx systemctl restart hsflowd
docker exec -d clab-netgo-host-3 ./ethr -s
docker exec -d clab-netgo-host-1 ./ethr -c 203.0.113.253 -b 900K -d 2m -p udp -l 1KB
docker exec -d clab-netgo-host-1 ./ethr -c 203.0.113.252 -b 600K -d 2m -p udp -l 1KB
docker exec -d clab-netgo-host-1 ./ethr -c 203.0.113.251 -b 400K -d 2m -p udp -l 1KB
$ sudo ip netns exec clab-netgo-host-2 /usr/local/go/bin/go run ch09/event-manager/main.go
AlertManager event-triggered webhook
2022/09/14 21:02:57 Starting web server at 0.0.0.0:10000
2022/09/14 21:02:58 Incoming alert
2022/09/14 21:02:58 swp1 needs to permit backup prefixes
2022/09/14 21:02:58 Created revisionID: changeset/cumulus/2022-09-14_21.02.58_S4SQ
{
  "state": "apply",
  "transition": {
    "issue": {},
    "progress": ""
  }
}
2022/09/14 21:03:40 Incoming alert
2022/09/14 21:03:40 swp1 needs to deny backup prefixes
2022/09/14 21:03:40 Created revisionID: changeset/cumulus/2022-09-14_21.03.40_S4SS
{
  "state": "apply",
  "transition": {
    "issue": {},
    "progress": ""
  }
}
2022/09/14 21:03:40 swp2 needs to permit backup prefixes
2022/09/14 21:03:40 Could not find a backup prefix for swp2
2022/09/14 21:04:10 Incoming alert
2022/09/14 21:04:10 swp1 needs to permit backup prefixes
2022/09/14 21:04:10 Created revisionID: changeset/cumulus/2022-09-14_21.04.10_S4SV
{
  "state": "apply",
  "transition": {
    "issue": {},
    "progress": ""
  }
}
2022/09/14 21:04:10 swp2 needs to deny backup prefixes
2022/09/14 21:04:10 Could not find a backup prefix for swp2
```

The output of the program would look like this:

```markup
ch09/gnoi-trace$ go run main.go
2022/09/14 21:03:29 Checking if routes have different paths
2022/09/14 21:03:34 Missed at least one hop in 203.0.113.253
2022/09/14 21:03:34 retrying 203.0.113.253
2022/09/14 21:03:34 Found different paths at hop 0
2022/09/14 21:03:34 Destination 203.0.113.252: Set{192.0.2.5}
2022/09/14 21:03:34 Destination 203.0.113.253: Set{192.0.2.3}
2022/09/14 21:03:34 Found different paths at hop 0
2022/09/14 21:03:34 Destination 203.0.113.251: Set{192.0.2.5}
2022/09/14 21:03:34 Destination 203.0.113.253: Set{192.0.2.3}
2022/09/14 21:03:34 Found different paths at hop 0
2022/09/14 21:03:34 Destination 203.0.113.253: Set{192.0.2.3}
2022/09/14 21:03:34 Destination 203.0.113.252: Set{192.0.2.5}
2022/09/14 21:03:34 Check complete
```

The last output shows that the path that `203.0.113.252/32` and `203.0.113.251/32` follow is different from the path that `203.0.113.253/32` follows (primary link). This is because the event-manager disaggregated `.252` and `.251` from the main `203.0.113.250/30` prefix. Now, we know that the backup link is working as expected, as it is carrying traffic for these two IP addresses.

Historically, networking vendors were not incentivized to create vendor-neutral APIs and data models, as it doesn’t allow them to differentiate themselves from the competition. And while standards bodies such as the Internet Engineering Task Force (IETF) produce standards for the networking industry, they can’t always influence what vendors actually implement. Also, some vendors might still perceive technological lock-ins as an effective way to keep their existing customer base.

By contrast, the OpenConfig community of network operators has more leverage to influence networking vendors to adopt vendor-independent data models and APIs. OpenConfig adoption is still relatively low, in both model and feature coverage, but, as long as the OC participants continue to push for more, the coverage will increase, which, in turn, will drive the adoption in the wider networking community.

Even today, OpenConfig provides a vendor-neutral way of doing a lot of common networking tasks, including configuration management, monitoring, and operations. In this chapter, we’ve shown the two most popular interfaces, gNMI and gNOI, ignoring the less common gRIBI, which is outside of the scope of this book. We hope this chapter provides enough examples of tools and workflows that you can use with Go to consume and interact with OpenConfig-compliant devices.

Bookmark

# Summary

In this chapter, by introducing streaming telemetry, we have started exploring the world of network monitoring, a critical task for a business. The ability to observe network-wide state and collect and process data plane information are all important in determining the health of your network. In the next chapter, we will examine a few concrete examples of network monitoring tasks and use cases and learn how Go can help us automate them.

Bookmark

# Further reading

-   Network operators: [https://www.openconfig.net/about/participants/](https://www.openconfig.net/about/participants/) 
-   Manipulating forwarding entries: https://github.com/openconfig/gribi/blob/master/doc/motivation.md#grpc-service-for-rib-injection
-   gNMI collector: https://github.com/openconfig/gnmi/tree/master/cmd/gnmi\_collector
-   gNMI CLI utility: [https://github.com/openconfig/gnmi/tree/master/cmd/gnmi\_cli](https://github.com/openconfig/gnmi/tree/master/cmd/gnmi_cli%20)
-   gNMI Test framework: [https://github.com/openconfig/gnmitest](https://github.com/openconfig/gnmitest%20)
-   gRPC tunnel: [https://github.com/openconfig/grpctunnel](https://github.com/openconfig/grpctunnel%20)
-   IS-IS LSDB parsing: [https://github.com/openconfig/lsdbparse](https://github.com/openconfig/lsdbparse%20)
-   Ygot: [https://github.com/openconfig/ygot](https://github.com/openconfig/ygot%20)
-   gNxI Tools: https://github.com/google/gnxi
-   Book’s GitHub repository: [https://github.com/ImagineDevOps DevOps/Network-Automation-with-Go](https://github.com/ImagineDevOps DevOps/Network-Automation-with-Go%20)
-   gNMI specification: [https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md](https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-specification.md)
-   gNMIc docs: https://gnmic.kmrd.dev/user\_guide/golang\_package/intro/#set-request
-   gNMI path convention: [https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-path-conventions.md](https://github.com/openconfig/reference/blob/master/rpc/gnmi/gnmi-path-conventions.md%20)
-   gNMI repository: [https://github.com/openconfig/gnmi](https://github.com/openconfig/gnmi%20)
-   gNMIc: [https://gnmic.kmrd.dev/](https://gnmic.kmrd.dev/%20)
-   AlertManager: [https://prometheus.io/docs/alerting/latest/alertmanager/](https://prometheus.io/docs/alerting/latest/alertmanager/%20)
-   `full/workdir/`: [https://github.com/ImagineDevOps DevOps/Network-Automation-with-Go/tree/main/topo-full/workdir](https://github.com/ImagineDevOps DevOps/Network-Automation-with-Go/tree/main/topo-full/workdir%20)
-   `topo.yml`: [https://github.com/ImagineDevOps DevOps/Network-Automation-with-Go/blob/main/topo-full/topo.yml](https://github.com/ImagineDevOps DevOps/Network-Automation-with-Go/blob/main/topo-full/topo.yml%20)
-   `docker-compose.yml`: [https://github.com/ImagineDevOps DevOps/Network-Automation-with-Go/blob/main/ch09/docker-compose.yml](https://github.com/ImagineDevOps DevOps/Network-Automation-with-Go/blob/main/ch09/docker-compose.yml%20)
-   gNOI GitHub repository: [https://github.com/openconfig/gnoi](https://github.com/openconfig/gnoi%20)
-   `system.proto` file: [https://github.com/openconfig/gnoi/blob/master/system/system.proto](https://github.com/openconfig/gnoi/blob/master/system/system.proto)