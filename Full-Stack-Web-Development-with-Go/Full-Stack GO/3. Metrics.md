# Application Metrics and Tracing

In [_Chapter 2_](https://subscription.imaginedevops.io/book/web-development/9781803234199/2B18295_02.xhtml#_idTextAnchor029), _Application Logging_, we looked at logging, and how we use logging inside our backend Go code. In this chapter, we will proceed to look at monitoring and tracing. To monitor and trace the application, we will look into different open source tools and libraries.

We have started building our application, and now we need to start looking into how we are going to support it. Once an application is running in production, we need to see what’s happening in the application. Having this kind of visibility will allow us to understand problems that come up. In software systems, we will often come across the concept of _observability_. The concept refers to the ability of software systems to capture and store data used for analysis and troubleshooting purposes. This includes the processes and tools used in order to achieve the goal of allowing users to observe what’s happening in the system.

In this chapter, we’ll be covering the following topics:

-   Understanding the OpenTelemetry specification
-   Tracing applications
-   Adding metrics to our application using Prometheus
-   Running `docker-compose`

Bookmark

# Technical requirements

All the source code explained in this chapter is available from GitHub here: [https://github.com/ImagineDevOps/Full-Stack-Web-Development-with-Go/tree/main/Chapter03](https://github.com/ImagineDevOps/Full-Stack-Web-Development-with-Go/tree/main/Chapter03).

We will be using another tool called OpenTelemetry, which will be explained in the next section, and the version that we use in this book is v1.2.0, available here: [https://github.com/open-telemetry/opentelemetry-go/tree/v1.2.0](https://github.com/open-telemetry/opentelemetry-go/tree/v1.2.0).

Bookmark

# Understanding OpenTelemetry

OpenTelemetry is an open source project that enables developers to provide observability capability to their applications. The project provides a Software Development Kit (SDK) for different programming languages, with Go as one of the supported languages, which is integrated with the application. The SDK is for metric collection and reporting, as it provides integration with different open source frameworks, making the integration process seamless. OpenTelemetry also provides a common standard, providing the application flexibility to report the collected data to different observability backend systems. OpenTelemetry’s website is at [https://opentelemetry.io/](https://opentelemetry.io/).

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.1_B18295.jpg)

Figure 3.1 – OpenTelemetry logo

**OpenTelemetry** is actually the merging of the OpenTracing and OpenCensus projects. The project is used to instrument, collect, and export metrics, logs, and traces. OpenTelemetry can be used across several languages, and Go is one of the supported languages.

The main benefit of following the OpenTelemetry specification is that it is vendor-agnostic, which means that applications written using their APIs are portable across different observability vendors. For example, applications that are written to write metrics into a filesystem will require a few lines of code changes to allow it to store metrics in Prometheus, which we will discuss in the _Adding metrics using_ _Prometheus_ section.

The two main components of OpenTelemetry are the following:

-   **Tracing**: This provides applications with the capability to track service requests as they flow through systems by collecting data. For example, with the tracing capability, we can see how an HTTP request flows through the different systems in the network.
-   **Metrics**: This provides applications with the ability to collect and store measurements for detecting performance anomalies and forecasting. For example, collecting metrics in our application will give us visibility into how long a database query takes or how long it takes to process a certain batch job.

You can find the OpenTelemetry specification at the following link: [https://opentelemetry.io/docs/reference/specification/](https://opentelemetry.io/docs/reference/specification/).

The specification allows users to plug-and-play different OpenTelemetry implementations easily without any dependency on single-vendor libraries. This means that all the relevant contracts that are outlined in the specification document can be implemented. Some concepts are important to understand in order to use OpenTelemetry effectively. The following are the concepts that are relevant to the specification:

-   **Components**: These are basically the core vendor-agnostic specifications, outlining the different parts of the system that need to be implemented. The components are collectors, the APIs, the SDK, and instrumenting libraries.
-   **Data sources**: This is the data that the specification supports: traces, logs, metrics, and baggage.
-   **Instrumenting and libraries**: There are two ways to integrate the provided library – either automatically by using the library provided by the vendor or open source contribution, or manually as per the application requirements.

In the next section, we are going to look at the implementation side of the specification, which involves both the APIs and the SDK.

## The OpenTelemetry APIs and SDK

OpenTelemetry is made of several components, and two of the main components that we are going to talk about are the APIs and SDK. The specification defines cross-language requirements that any implementation must adhere to as part of the requirements:

-   The **APIs**: This defines the data types and operations that will be used to generate telemetry data
-   The **SDK**: This defines the implementation of the APIs for processing and exporting capabilities

There is a clear distinction between the APIs and SDK – it’s clear that the APIs are contracts that are provided by the specification, while the SDK provides the different functionalities required to allow metrics data to be processed and exported. Metrics data contains information such as memory used, CPU usage, etc.

The specification provides an API for the following:

-   **Context**: This contains the values that are carried around across API calls. This is data that can be passed between system calls and carry application information.
-   **Baggage**: A set of name-value pairs describing user-defined properties.
-   **Tracing**: An API definition that provides the tracing functionality
-   **Metrics**: An API definition that provides the metric recording functionality

We will look at how the OpenTelemetry tracing API looks and how to add the tracing capability to applications.

Bookmark

# Tracing applications

In the previous chapter, we learned about logging and how logging can give us visibility into what’s going on inside our application. The line between logging and tracing is blurry; what we need to understand is that logging just provides information on what a process is currently doing, while tracing gives us cross-cutting visibility across different components, allowing us to get a better understanding of the data flow and time taken for a process to complete.

For example, with tracing, we can answer questions such as the following:

-   How long does the add-to-cart process take?
-   How long does it take to download a payment file?

We will go through the different APIs that are outlined in the specification and implement those APIs using the implementation provided by the OpenTelemetry library.

The following figure shows the links between different entities.

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.2_B18295.jpg)

Figure 3.2 – Tracing an API relationship

**TracerProvider** is the entry point to use the tracing API and it provides access to **Tracer**, which is responsible for creating **Span**. **Span** is used to trace an operation in our application. Before we move further to the next layer, which is the SDK, we will take a look briefly at **Jaeger**, which is one of the support tools provided by the OpenTelemetry library for tracing.

## Installing Jaeger

Jaeger ([https://www.jaegertracing.io/](https://www.jaegertracing.io/)) is a popular open source distributed tracing platform; it provides its own client libraries for a wide variety of programming languages, which can be seen at [https://github.com/orgs/jaegertracing/repositories](https://github.com/orgs/jaegertracing/repositories). We will be running Jaeger as a Docker container to reduce the amount of setup that is required when installing the application manually. Let’s start up Jaeger using the following `docker` command:

```markup
docker run --name jaeger \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 9411:9411 \
  jaegertracing/all-in-one:latest
```

On successful launch, there will be a lot of logs printed that look like the following:

```markup
{"level":"info","ts":1637930923.8576558,"caller":"flags/service.go:117","msg":"Mounting metrics handler on admin server","route":"/metrics"}
{"level":"info","ts":1637930923.857689,"caller":"flags/service.go:123","msg":"Mounting expvar handler on admin server","route":"/debug/vars"}
{"level":"info","ts":1637930923.8579082,"caller":"flags/admin.go:104","msg":"Mounting health check on admin server","route":"/"}
{"level":"info","ts":1637930923.8579528,"caller":"flags/admin.go:115","msg":"Starting admin HTTP server","http-addr":":14269"}
…
…
{"level":"info","ts":1637930923.8850179,"caller":"app/server.go:258","msg":"Starting HTTP server","port":16686,"addr":":16686"}
{"level":"info","ts":1637930923.8850145,"caller":"healthcheck/handler.go:129","msg":"Health Check state change","status":"ready"}
{"level":"info","ts":1637930923.8850334,"caller":"app/server.go:277","msg":"Starting GRPC server","port":16685,"addr":":16685"}
{"level":"info","ts":1637930924.8854718,"caller":"channelz/logging.go:50","msg":"[core]Subchannel Connectivity change to IDLE","system":"grpc","grpc_log":true}
{"level":"info","ts":1637930924.8855824,"caller":"grpclog/component.go:71","msg":"[core]pickfirstBalancer: UpdateSubConnState: 0xc00003af30, {IDLE connection error: desc = \"transport: Error while dialing dial tcp :16685: connect: connection refused\"}","system":"grpc","grpc_log":true}
{"level":"info","ts":1637930924.885613,"caller":"channelz/logging.go:50","msg":"[core]Channel Connectivity change to IDLE","system":"grpc","grpc_log":true}
```

Jaeger is now ready, the tool is not a desktop application but it provides a user interface that is accessible using the browser. Open your browser and type in the following URL: http://localhost:16686. It will open the Jaeger main page (_Figure 3__.3_):

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.3_B18295.jpg)

Figure 3.3 – Jaeger main page

At the moment, Jaeger does not contain anything, as there are no applications that are using it.

Bookmark

# Integrating the Jaeger SDK

Now that Jaeger is ready, let’s look at how we are going to write tracing information using OpenTelemetry. The library provides support for the Jaeger SDK out of the box; this allows applications to use the API to write tracing to Jaeger.

The example that we will be using in this section is inside the `jaeger/opentelem/trace` directory in the chapter’s GitHub repository. The file that we want to look at is `tracing.go` as shown here:

```markup
  package trace
  import (
           «context»
           «go.opentelemetry.io/otel"
           «go.opentelemetry.io/otel/exporters/jaeger"
           «go.opentelemetry.io/otel/sdk/resource"
           «go.opentelemetry.io/otel/sdk/trace"
           sc "go.opentelemetry.io/otel/semconv/v1.4.0"
  )
  type ShutdownTracing func(ctx context.Context) error
  func InitTracing(service string) (ShutdownTracing, error)
  {
    // Create the Jaeger exporter.
    exp, err := jaeger.New(jaeger.WithCollectorEndpoint())
    if err != nil {
     return func(ctx context.Context) error { return nil },
       err
    }
    // Create the TracerProvider.
    tp := trace.NewTracerProvider(
            trace.WithBatcher(exp),
            trace.WithResource(resource.NewWithAttributes(
                    sc.SchemaURL,
                    sc.ServiceNameKey.String(service),
            )),
    )
    otel.SetTracerProvider(tp)
    return tp.Shutdown, nil
  }
```

Let’s take a look at what each part of the code is doing. Line 18 is initializing the Jaeger SDK inside the OpenTelemetry library. On successfully initializing the Jaeger SDK, the code continues to provide the newly created Jaeger and uses it with the OpenTelemetry library to create a new `TracerProvider` API. As discussed in the previous section, `TracerProvider` is the API that is used as the main entry for OpenTelemetry. This is performed on lines 24-30.

On obtaining `TracerProvider`, we will need to call the global `SetTracerProvider` to let OpenTelemetry know about it, which is done on line 32. Once the Jaeger SDK has been successfully initialized, now it’s a matter of using it in the application.

Let’s take a look at the code sample for using the tracing functionality. The sample application that we are going to look at can be found inside the `jaeger/opentelem` directory inside `main.go`.

## Integration with Jaeger

We are going to go through section by section to explain what the code is doing. The following code section shows the `InitTracing` function that takes care the initialization process being called:

```markup
  package main
  import (
           t "chapter.3/trace/trace"
           "context"
           "fmt"
           "go.opentelemetry.io/otel"
           "go.opentelemetry.io/otel/attribute"
           "go.opentelemetry.io/otel/trace"
           "log"
           "sync"
           "time"
  )
  const serviceName = "tracing"
  func main() {
    sTracing, err := t.InitTracing(serviceName)
    if err != nil {
      log.Fatalf("Failed to setup tracing: %v\n", err)
    }
    defer func() {
      if err := sTracing(context.Background()); err != nil
      {
        log.Printf("Failed to shutdown tracing: %v\n", err)
      }
    }()
    ctx, span := otel.Tracer(serviceName)
                 .Start(context.Background(), "outside")
    defer span.End()
    var wg sync.WaitGroup
    wg.Add(1)
    go func() {
      _, s := otel.Tracer(serviceName).Start(ctx, "inside")
      ...
      wg.Done()
    }()
      wg.Add(1)
      go func() {
        _, ss := otel.Tracer(serviceName).Start(ctx,
                                                "inside")
       ...
        wg.Done()
      }()
      wg.Wait()
      fmt.Println("\nDone!")
  }
```

Once the SDK completes the initialization process, the code can start using the API to write tracing information and this is done by getting a `Span` using the `Tracer` API as shown on lines 27-29. The code uses `sync.WaitGroup` (lines 35 and 45) to ensure that the `main` thread does not finish before the goroutine completes – the goroutine is added to simulate some kind of processing to be done to generate a trace that will be reported to Jaeger.

The `Tracer` API only has one `Start` function, which is called to initiate the tracing operation, and the tracing operation is considered complete when the `End` function is called on `Span` – so, what is `Span`? `Span` is an API for tracing an operation; it has the following interface declaration:

```markup
type Span interface {
  End(options ...SpanEndOption)
  AddEvent(name string, options ...EventOption)
  IsRecording() bool
  RecordError(err error, options ...EventOption)
  SpanContext() SpanContext
  SetStatus(code codes.Code, description string)
  SetName(name string)
  SetAttributes(kv ...attribute.KeyValue)
  TracerProvider() TracerProvider
}
```

Multiple spans are pieced together to create a trace; it can be thought of as a **Directed Acyclic Graph** (**DAG**) of spans.

DAGs

A DAG is a term used in mathematics and computer science. It is a graph that shows dependencies, which, in our case, are the dependencies of application traces.

_Figure 3__.4_ shows what the composition of the trace looks like:

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.4_B18295.jpg)

Figure 3.4 – A DAG of a simple trace

The sample code creates two goroutines to perform a `sleep` operation and write trace information as shown below:

```markup
go func() {
  _, s := otel.Tracer(serviceName).Start(ctx, "inside")
  defer s.End()
  time.Sleep(1 * time.Second)
  s.SetAttributes(attribute.String("sleep", "done"))
  s.SetAttributes(attribute.String("go func", "1"))
  wg.Done()
}()
...
...
go func() {
  _, ss := otel.Tracer(serviceName).Start(ctx, "inside")
  defer ss.End()
  time.Sleep(2 * time.Second)
  ss.SetAttributes(attribute.String("sleep", "done"))
  ss.SetAttributes(attribute.String("go func", "2"))
  wg.Done()
}()
```

Run the complete sample application in `main.go` inside the `jaeger/opentelem` directory using the following command:

```markup
go run main.go
```

Upon completion, the application will write tracing information into Jaeger. Open Jaeger by accessing http://localhost:16686 in your browser. Once it’s opened, you will see a new entry under the **Service** dropdown as shown in _Figure 3__.5_:

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.5_B18295.jpg)

Figure 3.5 – Application trace search

The sample application tracing information is registered with the same string defined in the code, which is called `tracing`:

```markup
const serviceName = "tracing"
```

Clicking on the **Find Traces** button will read the trace information that is stored (_Figure 3__.6_):

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.6_B18295.jpg)

Figure 3.6 – Application traces

As can be seen in _Figure 3__.6_, there is only one entry and if you click on it, it will expand more information that the app has submitted via the `Span` API.

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.7_B18295.jpg)

Figure 3.7 – Tracing information

_Figure 3__.7_ shows the complete tracing information, which is a composition of spans from the application. Clicking on each of the graphs will bring up more information included in the span, which is included as shown in the code here:

```markup
go func() {
  ...
  s.SetAttributes(attribute.String("sleep", "done"))
  s.SetAttributes(attribute.String("go func", "1"))
  ...
}()
...
go func() {
  ...
  ss.SetAttributes(attribute.String("sleep", "done"))
  ss.SetAttributes(attribute.String("go func", "2"))
  ...
}()
```

Now that we know how to add tracing to our application, in the next section, we will look at adding metric instrumentation that will give us visibility into some of the performance metrics relevant to our application.

Bookmark

# Adding metrics using Prometheus

As OpenTelemetry is vendor-agnostic, it provides a wide variety of support for monitoring, exporting, and collecting metrics and one option is Prometheus. A complete list of different projects supported by OpenTelemetry can be found at [https://opentelemetry.io/registry/](https://opentelemetry.io/registry/). Prometheus is an open source monitoring and alerting system server that is widely used in cloud environments; it also provides libraries for a variety of programming languages.

In the previous section, we saw how to add tracing capabilities to our application and how to retrieve the traces by using Jaeger. In this section, we are going to take a look at how to create metrics using the `OpenTelemetry` library. Metrics allow us to get instrumentation information for our applications; it can provide answers to questions such as the following:

-   What is the total number of requests processed in service A?
-   How many total transactions are processed via payment gateway B?

Normally, collected metrics are stored for a certain amount of time to give us better insights into how the applications are performing by looking at a specific metric.

We will use the Prometheus open source project ([https://prometheus.io/](https://prometheus.io/)), which provides a complete monitoring solution stack and is very easy to use. The project provides a lot of features that are useful for collecting and storing metrics and monitoring our applications.

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.8_B18295.jpg)

Figure 3.8 – The Prometheus monitoring stack

Similar to tracing, the OpenTelemetry specification specifies the API and SDK for metrics, as shown in _Figure 3__.9_.

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.9_B18295.jpg)

Figure 3.9 – Metrics API

The following are explanations of the metrics APIs:

-   **MeterProvider**: This is an API for providing access to meters.
-   **Meter**: This is responsible for creating instruments, and is unique to the instrumentation in question.
-   **Instrument**: This contains the metric that we want to report; it can be synchronous or asynchronous.

## Adding metrics using Prometheus

Let’s start up Prometheus; make sure from your terminal that you are inside the `chapter3/prom/opentelem` directory and execute the following `docker` command:

```markup
docker run --name prom \
-v $PWD/config.yml:/etc/prometheus/prometheus.yml \
-p 9090:9090 prom/prometheus:latest
```

NOTE:

If you are using a Linux machine, use the following command:

```markup
 docker run --name prom \
 -v $PWD/config.yml:/etc/prometheus/prometheus.yml\
 -p 9090:9090  --add-host=host.docker.internal:host-gateway prom/prometheus:latest
```

The extra parameter, `--add-host=host.docker.internal:host-gateway`, will allow Prometheus to access the host machine using the `host.docker.internal` machine name.

The `config.yml` file used for configuring Prometheus is inside the `prom/opentelem` directory and looks like the following:

```markup
scrape_configs:
 - job_name: 'prometheus'
   scrape_interval: 5s
   static_configs:
     - targets:
         - host.docker.internal:2112
```

We will not go through the different available Prometheus configuration options in this section. The configuration we are using informs Prometheus that we want to get metrics from the container host, which is known internally in the container as `host.docker.internal`, at port `2112`, at an interval of 5 seconds.

Once Prometheus successfully runs, you will see the following log:

```markup
….
ts=2021-11-30T11:13:56.688Z caller=main.go:451 level=info fd_limits="(soft=1048576, hard=1048576)"
...
ts=2021-11-30T11:13:56.694Z caller=main.go:996 level=info msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
ts=2021-11-30T11:13:56.694Z caller=main.go:1033 level=info msg="Completed loading of configuration file" filename=/etc/prometheus/prometheus.yml totalDuration=282.112µs db_storage=537ns remote_storage=909ns web_handler=167ns query_engine=888ns scrape=126.942µs scrape_sd=14.003µs notify=608ns notify_sd=1.207µs rules=862ns
ts=2021-11-30T11:13:56.694Z caller=main.go:811 level=info msg="Server is ready to receive web requests."
```

Next, open your browser and type in the following: http://localhost:9090. You will be shown the main Prometheus UI:

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.10_B18295.jpg)

Figure 3.10 – The Prometheus UI

_Figure 3__.11_ shows the way Prometheus collects metrics via a pulling mechanism where it _pulls_ metric information from your application by connecting to port `2112`, which is exposed by the HTTP server running in the application. We will see later that most of the heavy lifting is done by the `OpenTelemetry` library; our application will just have to provide the metric that we want to report on.

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.11_B18295.jpg)

Figure 3.11 – Prometheus metric collection

Now that Prometheus is ready, we can start recording metrics to for our application. Run the application inside the `prom/opentelem` directory as follows:

```markup
go run main.go
```

Let the application run for a bit and you will see the following log:

```markup
2021/11/30 22:42:08 Starting up server on port 8000
2021/11/30 22:42:12 Reporting metric metric.random
2021/11/30 22:42:22 Reporting metric metric.random
2021/11/30 22:42:32 Reporting metric metric.random
2021/11/30 22:42:47 Reporting metric metric.random
2021/11/30 22:42:57 Reporting metric metric.random
```

-   `metric.totalrequest`: This metric reports the total number of requests processed by the application; the sample application has an HTTP server running on port `8000`
-   `metric.random`: This metric reports a random number

With the successful run of the sample application, we can now see the metric in the Prometheus UI. Open your browser and head to http://localhost:9090 and type in `metric_random` and you will see something such as that shown in _Figure 3__.12_; click on the **Execute** button.

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.12_B18295.jpg)

Figure 3.12 – metric\_random metric

Select the **Graph** tab and you will see the following figure:

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.13_B18295.jpg)

Figure 3.13 – metric\_random graph

The other metric that we want to show is the total number of requests processed by the sample application’s HTTP server. In order to generate some metrics, open the browser and enter http://localhost:8000; do so a few times so that some metrics will be generated.

Open the Prometheus UI again (http://localhost:9090), add the `metric_totalrequest` metric as shown in _Figure 3__.14_, and click on **Execute**:

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.14_B18295.jpg)

Figure 3.14 – metric\_totalrequest metric

The graph will look as follows:

![](https://static.packt-cdn.com/products/9781803234199/graphics/image/Figure_3.15_B18295.jpg)

Figure 3.15 – metric\_totalrequest graph

If you are having problems and cannot see the metrics, change the Prometheus configuration file, `config.yml`, inside the `chapter3/prom/opentelem` directory and change the target from `host.docker.internal` to `localhost` as shown here:

```markup
scrape_configs:
 - job_name: 'prometheus'
   scrape_interval: 5s
   static_configs:
     - targets:
     - localhost:2112
```

The `metrics.go` source contains the code that initializes the `otel` SDK to configure it for Prometheus, which is shown in the code snippet here:

```markup
package metric
...
type ShutdownMetrics func(ctx context.Context) error
// InitMetrics use Prometheus exporter
func InitMetrics(service string) (ShutdownMetrics, error) {
    config := prometheus.Config{}
    c := controller.New(
        processor.NewFactory(
            selector.NewWithExactDistribution(),
            aggregation.CumulativeTemporalitySelector(),
            processor.WithMemory(true),
        ),
        controller.WithResource(resource.NewWithAttributes(
            semconv.SchemaURL,
            semconv.ServiceNameKey.String(service),
        )),
    )
    exporter, err := prometheus.New(config, c)
    if err != nil {
      return func(ctx context.Context) error { return nil},
        err
    }
    global.SetMeterProvider(exporter.MeterProvider())
    srv := &http.Server{Addr: ":2112", Handler: exporter}
    go func() {
        _ = srv.ListenAndServe()
    }()
    return srv.Shutdown, nil
```

The following code snippet shows how it sends the metrics to Prometheus – the code can be found in `main.go` inside the `chapter3/prom/opentelem` directory:

```markup
package main
...
const serviceName = "samplemetrics"
func main() {
    ...
    //setup handler for rqeuest
    r.HandleFunc("/", func(rw http.ResponseWriter, r
      *http.Request) {
        log.Println("Reporting metric metric.totalrequest")
        ctx := r.Context()
        //add request metric counter
        ctr.Add(ctx, 1)
        ...
    }).Methods("GET")
    ...
}
```

Now that we have successfully added metrics and tracing to our applications and can view them using both Jaeger and Prometheus; in the next section, we will look at putting all the tools together to make it easy to run them as a single unit.

Bookmark

# Running docker-compose

We normally run containers using the `docker` command, but what if we want to run more than one container in one go? This is where `docker-compose` comes to the rescue. The tool allows you to configure the different containers that you want to run as a single unit. It also allows different kinds of configurations for different containers – for example, container A can communicate via the network with container B but not with container C.

The `docker-compose` tool that we are using in this book is v2, which is the recommended version. You can find instructions for installing the tool for different operating systems here – [https://docs.docker.com/compose/install/other/](https://docs.docker.com/compose/install/other/).

To make it easy to run both Prometheus and Jaeger, you can use `docker-compose`. The `docker-compose.yml` file looks as follows:

```markup
version: '3.3'
services:
 jaeger:
   image: jaegertracing/all-in-one:latest
   ports:
     - "6831:6831/udp"
     - "16686:16686"
     - "14268:14268"
 prometheus:
   image: prom/prometheus:latest
   volumes:
     -./prom/opentelem/config.yml:/etc/prometheus/
      prometheus.yml
   command:
     - '--config.file=/etc/prometheus/prometheus.yml'
     - '--web.console.libraries=/usr/share/prometheus/
       console_libraries'
     - '--web.console.templates=/usr/share/prometheus/
       consoles›
   ports:
     - 9090:9090
   network_mode: "host"
```

Run `docker-compose` using the following command:

```markup
docker-compose -f docker-compose.yml  up
```

On a successful run, you will see the following log:

```markup
prometheus_1  | ts=2021-12-04T07:45:02.443Z caller=main.go:406 level=info msg="No time or size retention was set so using the default time retention" duration=15d
prometheus_1  | ts=2021-12-04T07:45:02.443Z caller=main.go:444 level=info msg="Starting Prometheus" version="(version=2.31.1, branch=HEAD, revision=411021ada9ab41095923b8d2df9365b632fd40c3)"
prometheus_1  | ts=2021-12-04T07:45:02.443Z caller=main.go:449 level=info build_context="(go=go1.17.3, user=root@9419c9c2d4e0, date=20211105-20:35:02)"
prometheus_1  | ts=2021-12-04T07:45:02.443Z caller=main.go:450 level=info host_details="(Linux 5.3.0-22-generic #24+system76~1573659475~19.10~26b2022-Ubuntu SMP Wed Nov 13 20:0 x86_64 pop-os (none))"
prometheus_1  | ts=2021-12-04T07:45:02.444Z caller=main.go:451 level=info fd_limits="(soft=1048576, hard=1048576)"
prometheus_1  | ts=2021-12-04T07:45:02.444Z caller=main.go:452 level=info vm_limits="(soft=unlimited, hard=unlimited)"
jaeger_1      | 2021/12/04 07:45:02 maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
prometheus_1  | ts=2021-12-04T07:45:02.445Z caller=web.go:542 level=info component=web msg="Start listening for connections" address=0.0.0.0:9090
....
....
....
jaeger_1      | {"level":"info","ts":1638603902.657881,"caller":"healthcheck/handler.go:129","msg":"Health Check state change","status":"ready"}
jaeger_1      | {"level":"info","ts":1638603902.657897,"caller":"app/server.go:277","msg":"Starting GRPC server","port":16685,"addr":":16685"}
jaeger_1      | {"level":"info","ts":1638603902.6579142,"caller":"app/server.go:258","msg":"Starting HTTP server","port":16686,"addr":":16686"}
```

The `up` parameter we are using will start the container in the terminal and run in attached mode, which allows you to show all the logs on the screen. You also can run in detached mode to run the container in the background as follows:

```markup
docker-compose -f docker-compose.yml  up -d
```

Bookmark

# Summary

In this section, we looked at how to add metrics and tracing into an application using the `OpenTelemetry` library. Having this observability in an application will enable us to troubleshoot issues faster and also keep track of the performance of our application from the provided metrics. We also took a look at using two different open source projects that allow us to look at the data collected from our application.

In this chapter, we looked at the plumbing and infrastructure required to monitor and trace our application. In the next chapter, we will look at different aspects of building both dynamic and static content for our web application and how to package the application to make it easier to deploy anywhere.