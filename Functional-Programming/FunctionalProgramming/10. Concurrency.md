# Concurrency and Functional Programming

Concurrency is all around us, both in the real world as well as the virtual one. Humans can easily multitask (although we might not do a good job at either task). It’s entirely possible to drink a cup of coffee while you are reading this chapter or to run while listening to a podcast. For machines, concurrency is a complex undertaking, although a lot of that complexity can be hidden away by the programming language we choose.

Go was built to be a language with all the necessary tools a modern-day software engineer needs. As we are now in a world where CPU power is abundant for most intents and purposes, it’s only natural that concurrency was a main concern when developing the language, rather than having to bolt it on later. In this chapter, we are going to take a look at how functional programming can help with concurrency and, conversely, how concurrency can help with functional programming.

In this chapter, we are going to cover the following topics:

-   Why functional programming helps us write concurrent code
-   How to create concurrent functions (Filter, Map, and so on)
-   How to chain functions together concurrently using the pipeline pattern

Just Imagine

# Technical requirements

For this chapter, you can use any version of Go at or above version 1.18. All the code for this chapter can be found on GitHub at [https://github.com/ImagineDevOps DevOps/Functional-Programming-in-Go./tree/main/Chapter10](https://github.com/ImagineDevOps DevOps/Functional-Programming-in-Go./tree/main/Chapter10).

Just Imagine

# Functional programming and concurrency

We have already hinted at it throughout this book, but the ideas behind functional programming can help us write concurrent code. Typically, thinking about concurrency is a bit of a headache, even when a language has modern tools to support it, such as goroutines and channels. Before we dive too deep into this material, let’s first take a small detour as a refresher on what exactly we mean when we talk about concurrent code, and how it compares to parallelism and distributed computing.

## Concurrency, parallelism, and distributed computing

The terms _concurrency_, _parallelism_, and _distributed computing_ are, at times, used interchangeably. And while they are related, they are not exactly the same thing. Let’s just point out what we mean by concurrency first. **Concurrency** is what happens when our program can execute multiple tasks at the same time. For example, when we are playing a video game, typically a thread is playing audio, another one is processing input from the player, and another one is taking care of the internal game logic, updating the game state and performing the main game loop.

Video games have been around for a long time, and a game such as _DOOM_ works in this way. It’s also safe to say that people were not playing this on a computer with multiple cores available back in 1995. In other words, it’s possible for a single core to manage the execution of these distinct tasks and give the appearance of executing them at the same time. Exactly how this is done is beyond the scope of this book, but as a takeaway, just remember that the concurrency that we will mainly focus on is concurrency as defined previously – not the simultaneous execution of code, but the concurrent execution of code. One thing to note, though, is that concurrency can happen across multiple cores, or pipelines, as well. However, to keep things simple, we can imagine concurrency using a single core.

This brings us to the second term, **parallelism**. When we talk about a program executing in parallel, this means that multiple cores are performing a task simultaneously. You can not have parallelism without a physical means to run two tasks at the same time. The native Go mechanisms of channels and goroutines are focused on concurrency and not parallelism. This is an important distinction between the two. However, Go still lends itself to building out parallel algorithms.

To get an idea of what this looks like, there are a few packages available for Go that offer parallel solutions, such as the ExaScience Pargo package: [https://github.com/ExaScience/pargo](https://github.com/ExaScience/pargo). At the time of writing, this package is written in a pre-generics fashion, so do bear that in mind when looking through the code. In _Figure 10__.1_, the difference between concurrency and parallelism is highlighted by how the tasks get executed. Notably, the two tasks in the concurrent model are broken into multiple chunks, and each gets assigned CPU time in an alternating fashion.

![Figure 10.1: Concurrent (above) versus parallel (below) execution](https://static.packt-cdn.com/products/9781801811163/graphics/image/Figure_10.1_B18771.jpg)

Figure 10.1: Concurrent (above) versus parallel (below) execution

Finally, we have **distributed computing**. While concurrency is part of distributed computing, it is not the only requirement for this. Distributed computation does imply spreading out computational tasks over multiple machines, in which sense it is concurrent, but there’s more overhead than with typically concurrent or parallel applications.

In distributed systems, you need to have mechanisms for fault tolerance (what if one node in the network becomes unavailable?) and mechanisms for dealing with the network (unreliable or insecure networks). So, while people might talk about distributed computation as an example of concurrency, concurrency only gives you the bare minimum required. The physical infrastructure and myriad of difficulties in making a distributed system work are beyond the scope of this book. One thing to take away is that Go is a language that can be used to write distributed systems. In fact, the use of goroutines and channels might help you build out the underlying infrastructure needed for distributed systems, but you’ll need more than the basic functionality of the language. If you want to learn more about distributed computing with Go, the book _Distributed Computing with Go_ is a good place to start: [https://www.imaginedevops.io/product/distributed-computing-with-go/9781787125384?\_ga=2.217817046.1391922680.1675144438-1944326834.1674539572](https://www.imaginedevops.io/product/distributed-computing-with-go/9781787125384?_ga=2.217817046.1391922680.1675144438-1944326834.1674539572).

In this chapter, we will focus on concurrency only, and we won’t zoom in on parallelism or distributed computing. However, why do we want our code to be concurrent? There are a few clear advantages that this can bring:

-   **Higher responsiveness**: A program does not need to wait for a single long-running task to complete before starting another one
-   **Higher performance**: If we can chunk out a heavy workload and perform this over multiple threads (and Go might schedule these across multiple cores to get a form of parallelism as well), this will reduce the time it takes to complete the operation

## Functional programming and concurrency

I’ve made the claim before in this book that functional programming makes it easier to write concurrent code, but this claim needs to be tailored a little bit further. When talking about how functional programming makes concurrency easier, we are talking about the stricter subset of functional programming called “pure” functional programming. Pure functional programming gives us a few key features that make reasoning about concurrent execution easier and our code less error-prone. These are the main features responsible for this:

-   Immutable variables and state
-   Pure functions (no side effects)
-   Referential transparency
-   Lazy evaluation
-   Composability

For the rest of this chapter, when talking about functional programming, the assumption can be made that we’re talking strictly about pure functional programming. Let’s focus on each of these features and explain why they make for safer concurrent code, or make our code at least easier to reason about. The result is that when our code is easier to understand, it should help us reduce the number of bugs in it.

### Immutable variables and state

When working in an object-oriented model, objects typically hold an internal state. If this state is allowed to mutate, then the state that two threads are working on might diverge. By not allowing the state to change, even if operating on the same data sources (or, rather, copies of the same data), our functions can execute independently of each other without ever messing with the shared memory.

In Go, if we do want to use structs, there are some pitfalls, which we discussed in earlier chapters. By avoiding the use of pointers, we can avoid the main causes of mutation in structs. When writing pure functional code, each individual component of our code needs to be immutable. When each component is immutable, we can more safely execute functions concurrently.

Another issue we avoid by having immutable variables and states is that of resource contention. If we have a single true resource (a singleton in an object-oriented model), then this resource might be locked by thread A, causing thread B to wait until the resource is freed up before it can be used. Typically, this is implemented through a resource-locking mechanism (thread A locks the resource , _X_, performs operations while other threads wait for resource _X_, and then finally removes the lock when it is done operating). In a purely functional world, we would not need such singleton operations, partly due to our immutable state and partly due to the other benefits, such as pure functions.

### Pure functions

As we saw in [_Chapter 4_](https://subscription.imaginedevops.io/book/programming/9781801811163/2B18771_04.xhtml#_idTextAnchor060), a function is considered pure when it does not produce any side effects and does not interact with the outside world. In this book, we implemented many functions that are common to functional programming. All of these were written in the pure functional style (although remember that pure functional is a subset of functional programming and not strictly required). The benefits here relate to the immutable state but extend beyond it as well. If our functions do not depend on the program state, then anything modifying the state of our program cannot disrupt our function.

Beyond this, it also eliminates another class of problems. If our functions were allowed to mutate state, or our system, the order of operations would matter. For example, imagine that we were to write a concurrent function that appends content to a file. Writing to a file is a clear case of a side effect, but in a concurrent application, the content of our file would now depend on the order in which our threads are executed. This breaks the determinism of our application and, furthermore, would likely lead to a file that is not exactly what we desired. In an object-oriented model, this is again resolved through locking. In a purely functional language, the “impure” functions would be handled by monads. Go is not purely functional, but later in this chapter, we will look at the pipeline pattern through which we can model the data flow and control the side effects.

### Referential transparency

**Referential transparency** means that we can replace a function call with its result, without changing the result of our computation. We covered this in more detail in [_Chapter 2_](https://subscription.imaginedevops.io/book/programming/9781801811163/2B18771_02.xhtml#_idTextAnchor028), but for concurrency, the important aspect is that if all our calls are referentially transparent, it does not matter when exactly a call is resolved (ahead of time or just in time). This means that when we chunk our code out into concurrent functions, it is safe to resolve certain function calls ahead of time in a concurrent fashion.

### Lazy evaluation

**Lazy evaluation** is a common approach when writing concurrent code. An example we are all too familiar with is the idea of _callbacks_. These are functions that can be passed to an asynchronous call, but they are only executed once they become relevant. It’s also entirely possible for a function to never get called. For example, let’s write a piece of code that performs an asynchronous `GET` request to a URL. We will use two callbacks, which will be lazily evaluated. The first callback will be resolved only if the `GET` request completed successfully, while the second callback will be resolved if the `GET` request failed. Note that here we mean the `GET` request itself did work, but we received a response code that is not in the `200` range:

```markup
import (
        "fmt"
        "io/ioutil"
        "net/http"
)
type ResponseFunc func(*http.Response)
func getURL(url string, onSuccess, onFailure ResponseFunc)
    {
        resp, err := http.Get(url)
        if err != nil {
                panic(err)
        }
        if resp.StatusCode >= 200 && resp.StatusCode < 300 {
                onSuccess(resp)
        } else {
                onFailure(resp)
        }
}
```

In the preceding code, we can see that `getURL` requires a string representing a URL to resolve, as well as two functions. Both functions have the same `ResponseFunc` type, which is a function with the `func(*http.Response)` signature.

Next, we can write a `main` function in which we call `getURL` and provide two callbacks:

-   The first callback, `onSuccess`, will be executed if our `GET` request returns a status code in the `200` range; this function will simply print out the content of the response body.
-   The second callback, `onFailure`, will simply print an error message along with the corresponding status code that our response received. We’ll call `getURL` twice, once with a valid URL and once with an invalid URL. However, instead of running this code synchronously, we will make the calls to `getURL` on separate goroutines by prefixing each call with `go`. This means we don’t know which call will complete first, but as we are using lazy functions (a type of continuation-passing style programming), we don’t have to orchestrate the control flow of our program. The correct callback will execute when its time comes. The callback, which is not necessary, will never be evaluated, so we avoid potentially expensive computation when it is not necessary:
    
    ```markup
    func main() {
    ```
    
    ```markup
            success := func(response *http.Response) {
    ```
    
    ```markup
                    fmt.Println("success")
    ```
    
    ```markup
                    content, err := ioutil.ReadAll
    ```
    
    ```markup
                        (response.Body)
    ```
    
    ```markup
                    if err != nil {
    ```
    
    ```markup
                            panic(err)
    ```
    
    ```markup
                    }
    ```
    
    ```markup
                    fmt.Printf("%v\n", string(content))
    ```
    
    ```markup
            }
    ```
    
    ```markup
            failure := func(response *http.Response) {
    ```
    
    ```markup
                    fmt.Printf("something went wrong,
    ```
    
    ```markup
                      received: %d\n", response
    ```
    
    ```markup
                        .StatusCode)
    ```
    
    ```markup
            }
    ```
    
    ```markup
            go getURL("https://news.ycombinator.com",
    ```
    
    ```markup
              success, failure)
    ```
    
    ```markup
            go getURL("https://news.ycombinator.com/
    ```
    
    ```markup
              ThisPageDoesNotExist", success, failure)
    ```
    
    ```markup
            done := make(chan bool)
    ```
    
    ```markup
            <-done // keep main alive
    ```
    
    ```markup
    }
    ```
    

In the preceding example, our `GET` requests complete asynchronously and then call the corresponding callback, as defined in the `getURL` function. One interesting bit of code is near the end of our main snippet. We have created a `bool` channel, and then we are reading from this channel without ever writing to it. This essentially keeps our application alive. If we didn’t have these two statements, our `main` function would likely exit and thus terminate our program, before our goroutines completed their computation. In a real-world application, you could also keep waiting for the threads to resolve using `waitgroup`. If you are stuck after running this from a terminal, press _Ctrl_ + _C_ to kill the process.

Lazy evaluation will show up again later in this chapter when we take a look at implementing functional pipes. However, we’ll be looking at it more through a direct lens of concurrent applications, rather than the callback mechanisms that we saw here.

Threads versus goroutines

While the terms _thread_ and _goroutine_ are often used interchangeably, they are distinct things. Goroutines are a construct in Go, built to leverage executing tasks concurrently. They are managed by the Go runtime, are lightweight and fast to start and execute, and have a built-in communication medium (channels). Threads, on the other hand, are implemented at the hardware level and are managed by the operating system. They are slower to spin up, have no communication medium built in, and are hardware-dependent.

### Composability

Functions are composable in a myriad of ways. This allows us to define the building blocks of our application and then chain them together to solve our concrete problem. As each block is independent of one another, we can build concurrency layers in between them. This will be the focus in the last part of this chapter when we will create functional pipes that can run concurrently. However, before we get there, let’s take a look at making our functions internally concurrent.

Just Imagine

# Creating concurrent functions

Broadly speaking, there are two types of concurrency that we will be looking at in this chapter. We can call them **intra-concurrency** and **extra-concurrency**:

-   _Intra-concurrency_ is about creating functions that are implemented concurrently internal to each function. For example, in [_Chapter 6_](https://subscription.imaginedevops.io/book/programming/9781801811163/2B18771_06.xhtml#_idTextAnchor101), we saw various functions such as `Filter`, `Map`, and `FMap` that lend themselves to a concurrent implementation. That will be the focus of this section. Notably, they can be used in conjunction with each other so that we achieve concurrency at multiple steps in our algorithm, and we can even decide on the level of concurrency required for each step individually.
-   _Extra-concurrency_ is about chaining together functions using Go's built-in concurrency features: channels and goroutines. This is explored later in the chapter.

Why are many of the fundamental building blocks of functional programming good candidates for concurrency? Well, first and foremost, it is because a purely functional implementation lends itself to a concurrent implementation without too many headaches. As we saw in the preceding chapter, concepts such as an immutable state and the elimination of side effects make it easy to take our functions and concurrently rewrite them. There should not be interference from other functions, no outside state to deal with, and no I/O to contend with. However, just because we _can_ does not mean that we _should_. In this chapter, I will make the assumption that a concurrent implementation is going to be the right choice for the problems that we are solving. In the real world, concurrency is not a zero-cost implementation. There is real overhead associated with writing a concurrent application, as the threaded execution needs to be managed by our system (or, in Go’s case, our runtime).

Although in Go we are not responsible for managing the goroutines ourselves, under the hood of the Go runtime, context switching is not a zero-cost implementation. This means that just adding concurrent calls does not guarantee a performance improvement and can, in fact, harm performance. Ultimately, as with anything done for performance, the key to understanding the benefit that can be achieved is obtained through profiling your application. Profiling itself is beyond the scope of this section; the only comment to make on it is that Go has built-in benchmarking tools, which we saw in earlier chapters. These can also be used to determine the cost benefit of concurrent versus sequential functions.

## Concurrent filter implementation

As we started with sequential filter implementation in earlier chapters and have become more familiar with it throughout the book, let’s start with this function and turn it into a concurrent implementation. Keep in mind that our initial function was a pure function, and as such, refactoring it into a concurrent one can be done without causing too much of a headache. There are a few steps to making this concurrent:

1.  Split the input into batches.
2.  Start a process to filter each batch.
3.  Aggregate the result of each batch.
4.  Return the aggregated output.

To achieve this, we do need to refactor the initial `Filter` implementation. We will leverage some of Go’s built-in concurrency features to implement this, and the first thing we’ll want to leverage are channels and goroutines. In our initial `Filter` function, we iterated over each element, appended it to an output slice if it matched the predicate, and finally, we returned the output slice. In this version, rather than returning an output slice, we’ll write the result onto a channel:

```markup
type Predicate[A any] func(A) bool
func Filter[A any](input []A, p Predicate[A], out chan []A)
  {
        output := []A{}
        for _, element := range input {
                if p(element) {
                        output = append(output, element)
                }
        }
        out <- output
}
```

Writing to a channel allows us to call this function in a traditional concurrent fashion within Go. However, before we get there, we’ll have to establish a wrapper function around `Filter`, which we will call `ConcurrentFilter`. This function does a few things, including allowing us to configure the batch size. Playing around with the batch sizes can help us tweak the performance to get it where we want it (if there are too few batches, there’s little benefit to running concurrently; too many, and the overhead caused by managing goroutines similarly reduces our benefit). Apart from batching our input, we’ll also need to call the `Filter` function prepended with the `go` keyword so that it spins up a new goroutine. Finally, this function will read the results for each of the goroutines that we started and aggregate this result in to a single output slice:

```markup
func ConcurrentFilter[A any](input []A, p Predicate[A],
batchSize int) []A {
output := []A{}
out := make(chan []A)
threadCount := int(math.Ceil(float64(len(input)) /
float64(batchSize)))
fmt.Printf("goroutines: %d\n", threadCount)
for i := 0; i < threadCount; i++ {
fmt.Println("spun up thread")
if ((i + 1) * batchSize) < len(input) {
go Filter(input[i*batchSize:(i+1)*batchSize],                       p, out)
} else {
go Filter(input[i*batchSize:], p, out)
}
}
for i := 0; i < threadCount; i++ {
filtered := <-out
fmt.Printf("got data: %v\n", filtered)
output = append(output, filtered...)
}
close(out)
return output
}
```

In the preceding code snippet, we keep the print statements so we can see what execution looks like when running this. Let’s create a simple `main` function that will filter a slice of integers in this fashion and look at the corresponding output:

```markup
func main() {
        ints := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
        output := ConcurrentFilter(ints, func(i int) bool {
           return i%2 == 0 }, 3)
        fmt.Printf("%v\n", output)
}
```

Running this function gives us the following output:

```markup
goroutines: 4
spun up thread
spun up thread
spun up thread
spun up thread
got data: [10]
got data: [2]
got data: [4 6]
got data: [8]
[10 2 4 6 8]
```

In this output, we can see that `4` goroutines had to be spun up to process our input with a batch size of `3`. This has sharded our input data into the following segments:

```markup
[]int{1,2,3}
[]int{4,5,6}
[]int{7,8,9}
[]int{10}
```

Next, we can see in which order the threads completed and returned their output. As you can tell from the output, we get the output back in random order. This is visible both in the `got data` output as well as in the final aggregated result.

Tip

An important callout here is that by sharding our data and running our functions concurrently, we no longer have a predictable sequence order in the output list. If we want to restore the ordering of our data, we should implement a `Sort` function after concurrently calling our functions.

This `Filter` implementation is a good template to start from when we want to make our functions run concurrently. Let’s take a look at a concurrent implementation for both the `Map` and `FMap` functions.

## Concurrent Map and FMap implementation

Implementing the `Map` and `FMap` functions concurrently requires the same steps as for the concurrent `Filter` implementation, as follows:

1.  Split the input into batches.
2.  Start a process to filter each batch.
3.  Aggregate the result of each batch.
4.  Return the aggregated output.

As such, we won’t go over each step in detail for these implementations. The explanation behind each step and how we implement it is pretty much identical to the `Filter` implementation. We are showing these here for completeness and to showcase the general pattern of refactoring these functions to operate concurrently.

### Concurrent Map

To implement our `Map` function concurrently, we first refactor the `Map` function that we created in [_Chapter 6_](https://subscription.imaginedevops.io/book/programming/9781801811163/2B18771_06.xhtml#_idTextAnchor101). Here, again, we are removing the explicit return, and we’ll use channels to communicate the output of mapping each element:

```markup
type MapFunc[A any] func(A) A
func Map[A any](input []A, m MapFunc[A], out chan []A) {
        output := make([]A, len(input))
        for i, element := range input {
                output[i] = m(element)
        }
        out <- output
}
```

Next, we will implement the `ConcurrentMap` function, batching the output as we did with the `ConcurrentFilter` implementation:

```markup
func ConcurrentMap[A any](input []A, mapFn MapFunc[A],
    batchSize int) []A {
        output := make([]A, 0, len(input))
        out := make(chan []A)
        threadCount := int(math.Ceil(float64(len(input)) /
            float64(batchSize)))
        fmt.Printf("goroutines: %d\n", threadCount)
        for i := 0; i < threadCount; i++ {
                fmt.Println("spun up thread")
                if ((i + 1) * batchSize) < len(input) {
                        go Map(input[i*batchSize:(i+1)
                           *batchSize], mapFn, out)
                } else {
                        go Map(input[i*batchSize:],
                            mapFn, out)
                }
        }
        for i := 0; i < threadCount; i++ {
                mapped := <-out
                fmt.Printf("got data: %v\n", mapped)
                output = append(output, mapped...)
        }
        close(output)
        return output
}
```

Note that both the `ConcurrentFilter` and `ConcurrentMap` implementations require `batchSize` to be passed as input to the function. This means that we can process each step with a different number of goroutines, and tweak each function individually:

```markup
func main() {
        ints := []int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}
        output := ConcurrentFilter(ints, func(i int) bool {
            return i%2 == 0 }, 3)
        fmt.Printf("%v\n", output)
        output = ConcurrentMap(output, func(i int) int {
            return i * 2 }, 2)
        fmt.Printf("%v\n", output)
}
```

In this example, we are using a batch size of `3` for filtering but only a batch size of `2` for mapping. The output of this `main` function looks like this:

```markup
goroutines: 4
spun up thread
spun up thread
spun up thread
spun up thread
got data: [10]
got data: [2]
got data: [4 6]
got data: [8]
[10 2 4 6 8]
{next statements are the output for the map function}
goroutines: 3
spun up thread
spun up thread
spun up thread
got data: [16]
got data: [20 4]
got data: [8 12]
[16 20 4 8 12]
```

### Concurrent FMap implementation

This implementation is pretty similar to the `Map` implementation. The main difference is that our channel has changed type. Rather than having the entire function signature operate on the same `A` type, we’ll now have a mix of `A` and `B`. This is a minor change and does not affect the implementation details beyond having to create the right type for the channels:

```markup
func FMap[A, B any](input []A, m func(A) B, out chan []B) {
        output := make([]B, len(input))
        for i, element := range input {
                output[i] = m(element)
        }
        out <- output
}
func ConcurrentFMap[A, B any](input []A, fMapFn ,
    batchSize int) []B {
        output := make([]B, 0, len(input)
        out := make(chan []B)
        threadCount := int(math.Ceil(float64(len(input)) /
            float64(batchSize)))
        fmt.Printf("goroutines: %d\n", threadCount)
        for i := 0; i < threadCount; i++ {
                fmt.Println("spun up thread")
                if ((i + 1) * batchSize) < len(input) {
                        go FMap(input[i*batchSize:
                           (i+1)*batchSize], fMapFn, out)
                } else {
                        go FMap(input[i*batchSize:],
                            fMapFn, out)
                }
        }
        for i := 0; i < threadCount; i++ {
                mapped := <-out
                fmt.Printf("got data: %v\n", mapped)
                output = append(output, mapped...)
        }
        return output
}
```

I hope that this serves as an illustration of how easy it is to create concurrent implementations for functions that are written in the purely functional style. There is one limitation posed by Go that makes this a bit more verbose than it would be in other languages. As Go is a strictly typed language (which is a good thing in general), our function signatures need to match exactly when using higher-order functions. Otherwise, we could template out the recursive part of our function and call a higher-order function for the actual implementation on each node. In pseudo-code, we would get something like the following:

```markup
func ConcurrentRunner(input []Input, fn func(), batchSize
  int) []Output {
     // set up channels and batch logic
     for batch in batches {
         go Run(fn(batch))
     }
     // collect output and return
}
```

Either way, we saw that leveraging concurrency in our functions is relatively headache-free and can be achieved with only a bit of refactoring. Let’s move on to the final topic of this chapter, which is using concurrency mechanisms to chain functions together.

Just Imagine

# The pipeline pattern

In the previous sections, we concerned ourselves with organizing concurrency inside the functions themselves. However, we have chained them together pretty much as we would normally, by calling them in sequential order in the main function. In this section, we are going to look at the pipeline pattern, which will allow us to leverage goroutines and channels to chain function calls together. First, let’s discuss what a pipeline is exactly. In 1964, Doug McIlroy wrote the following:

We should have some ways of coupling programs like garden hose – screw in another segment when it becomes necessary to massage data in another way.

This quote neatly expresses the Unix philosophy of composing programs. Many of us are familiar with the concept of Unix pipes, denoted by the `|` symbol. By using pipes, we can chain Unix programs together. The output of one program becomes the input of the next. For example, we can use `cat` to read a file, and we can use `wc` to get the word count of that file. To join this together, we would write `cat file.txt | wc`. In Unix’s modular program approach, the idea is that programs each serve a single purpose but can be joined together to create complex programs. This philosophy can be ported over to the functional programming paradigm. We want to chain simple functions together, where each function only has a single purpose, to create a complex program. Take the following example; each function serves a single purpose, and we chain them together using the pipe (`|`) character:

```markup
cat main.go | grep "func" | wc -l | awk '{print "lines: "
  $1}'
```

In this example, we first read the `main.go` file using `cat`. We send the content of that file to `grep`, which searches that content for the `func` keyword. Then, we send each line that matches this search to the `wc` program and count the lines in the output (the `-l` flag counts newlines). And finally, we send this to `awk` and print the result. What follows is a similar way of chaining Go functions together, rather than Unix commands.

### Chaining functions with channels

Go ships with all the tools necessary to create such building programs, namely channels. Channels are a way to send messages (data) from one function to another; thus, we can use channels as an alternative to the Unix pipes.

The first step in creating our pipeline starts by changing how our functions get their input and output. For the rest of this chapter, we will mainly be focusing on two functions, `Filter` and `Map`, but this can be extended to any other functions. The core idea is to use channels for input and output data communication. First, let’s take a look at the `Filter` function and how this needs to be changed to follow our channels-in/channels-out approach. We’ll name our new function `FilterNode`. We’ll get back to this naming convention later, but each function can be thought of as a node in our chain of functions. Instead of accepting a slice as input, we’ll have a channel as input, from which we can read incoming data. We’ll still have `predicate` as expected, and finally, we’ll return a channel rather than a slice of data as well:

```markup
func FilterNode[A any](in <-chan A, predicate Predicate[A])
  <-chan A {
        out := make(chan A)
        go func() {
                for n := range in {
                        if predicate(n) {
.                                out <- n
                        }
                }
                close(out)
        }()
        return out
}
```

In the preceding function, the main algorithm for filtering elements remains unchanged. We’ll test each value against a predicate; if the predicate returns `true`, we’ll keep the value (by sending it to the output channel); otherwise, we'll discard it. Pay attention to the use of the `go` keyword here. This function, although it gets executed immediately, is launched on its own goroutine. The function immediately returns the `out` channel, although the evaluation on the goroutine has not necessarily finished the computation.

The next function that we will refactor similarly is the `Map` function. It’s an analogous change to the `Filter` function. We’ll use a channel to receive input for the function, a channel to return the output, and run the actual mapping logic inside a goroutine, which we start before returning the channel from our function:

```markup
func MapNode[A any](in <-chan A, mapf MapFunc[A]) <-chan A
  {
        out := make(chan A)
        go func() {
                for n := range in {
                        out <- mapf(n)
                }
                close(out)
        }()
        return out
}
```

So far, so good – we’ve refactored two of our functions to fit in with this new design. Next, let’s tackle the question of receiving input to these functions. From the function signature, we can tell that we need to receive data on a channel of type `A`. Thus, any function that can provide this can be used as the input for our function. We’ll call these types of functions _generators_. The first generator that we will create takes a variadic input of type `A` and pushes each of these values onto a channel:

```markup
func Generator[A any](input ...A) <-chan A {
        out := make(chan A)
        go func() {
                for _, element := range input {
                        out <- element
                }
                close(out)
        }()
        return out
}
```

As you can tell, the main logic still resembles that of the previous `Filter` and `Map` implementations. The main difference is that we’re no longer receiving values over a channel but, rather, through some other input data structure (in this case, variadic input parameters). This could also be a function that reads a file and places each line on the channel. It’s similar to how `cat` worked in our earlier Unix example:

```markup
func Cat(filepath string) <-chan string {
        out := make(chan string)
        f, err := ioutil.ReadFile(filepath)
        if err != nil {
                panic(err)
        }
        go func() {
                lines := strings.Split(string(f), "\n")
                for _, line := range lines {
                        out <- line
                }
                close(out)
        }()
        return out
}
```

The key point is that our function places values on a channel and returns this channel. How it gets to those values doesn’t matter too much for building our pipeline. Before we can test this implementation end to end, we still have one hurdle to cross. Each node in this setup writes data to a channel, but to collect the output at the end, we’ll want to store it in a more common data structure. Slices are the perfect structure for this, at least in our examples. We can call this last type of function a _collector_. A collector takes a channel as input and returns a slice of the elements as output. Essentially, it’s performing the reverse operation of the _generator_:

```markup
func Collector[A any](in <-chan A) []A {
        output := []A{}
        for n := range in {
                output = append(output, n)
        }
        return output
}
```

With this in place, we can tie all of them together into a single pipeline. To demonstrate this, in our `main` function, we will push some numbers to a channel using `Generator`. We’ll then filter these numbers to only retain the even ones using `FilterNode`. These numbers then get squared using `MapNode`, and finally, we collect the output in a slice using the `Collector` function:

```markup
func main(){
        generated := Generator(1, 2, 3, 4)
        filtered := FilterNode(generated, func(i int) bool
            { return i%2 == 0 })
        mapped := MapNode(filtered, func(i int) int {
            return i * 2 })
        collected := Collector(mapped)
        fmt.Printf("%v\n", collected)
}
```

The output of running this is as follows:

```markup
[4 8]
```

The preceding is a good first step toward chaining our functions together. However, we can make it cleaner. We can build a `ChainPipes` function that will tie together the various functions and take care of managing the channels for us.

### Improved function chaining

The initial approach of chaining the functions together was a workable solution, but it required some overhead, as we had to manage passing around the right channels to each subsequent function. What we want to achieve is for the engineers using our setup only needing to concern themselves with the functions to call, and which order to call them in. We don’t want them to be concerned about how the channels operate underneath; we can consider that an implementation detail. What we will work toward in this section will allow us to compose the functions like so:

```markup
        out := pkg.ChainPipes(generated,
                pkg.CurriedFilterNode(func(i int) bool { return i%2 == 0 }),
                pkg.CurriedMapNode(func(i int) int { return
                    i * i }))
```

This snippet gives us a bit of a teaser of what’s to come. In order to chain functions like this, we will need to take advantage of function currying. Let’s get there step by step. What we want to achieve is function composition by passing in functions to `ChainPipes`, as we saw in the preceding snippet. Go has a strict type system, so to make this function work nicely, we want to define a custom type for such functions, which will allow us to use it in the function signature and get the compiler to type-check for us.

The first thing we will do is define our own types for the main functions representing an operation on our data. We’ll call these `Nodes`. There are three distinct types of nodes that we can define, based on the previous discussion – nodes that generate a channel, nodes that take a channel and return a new channel, and finally, nodes that take a channel and return a concrete data structure such as a slice:

```markup
type (
        Node[A any]          func(<-chan A) <-chan A
        GeneratorNode[A any] func() <-chan A
        CollectorNode[A any] func(<-chan A) []A
)
```

These type definitions make up the bread and butter of the function types that can be used to chain together our applications. With this in place, we can define the `ChainPipes` function as follows:

```markup
func ChainPipes[A any](in <-chan A, nodes ...Node[A]) []A {
        for _, node := range nodes {
                in = node(in)
        }
        return Collector(in)
}
```

The preceding snippet creates a `ChainPipes` function that takes a channel as input and a series of nodes. Finally, it will call the default collector and return the data in a slice of type `[]A`. Do note that one limitation is that we are assuming that each node has a compatible type (`A`) throughout the chain.

To make the type system work, each node needs to have the same function signature. In our initial setup, that was difficult, as we already had two distinct function signatures for `Filter` and `Map`:

```markup
func FilterNode[A any](in <-chan A, predicate Predicate[A])
  <-chan A
func MapNode[A any](in <-chan A, mapf MapFunc[A]) <-chan A
```

More functions would mean more distinct function signatures. Therefore, what is needed is refactoring these functions so that they adhere to the same type signature. We have already learned how to do that through function currying. We need to create two new functions that each **return** a function of type `Node`. Each function will have the original functionality of `Filter` and `Map` baked in but returns a new function that takes a channel as the input (hence the function is partially applied):

```markup
func CurriedFilterNode[A any](p Predicate[A]) Node[A] {
        return func(in <-chan A) <-chan A {
                out := make(chan A)
                go func() {
                        for n := range in {
                                if p(n) {
                                        out <- n
                                }
                        }
                        close(out)
                }()
                return out
        }
}
func CurriedMapNode[A any](mapFn MapFunc[A]) Node[A] {
        return func(in <-chan A) <-chan A {
                out := make(chan A)
                go func() {
                        for n := range in {
                                out <- mapFn(n)
                        }
                        close(out)
                }()
                return out
        }
}
```

We can tell in the preceding example that the core logic of each function has remained the same. However, rather than being instantly applied when the function is called, a new function is returned that expects to receive a channel as input and returns a channel as output. Inside this anonymous function, we have coded the `Filter` and `Map` logic respectively.

As the return type is `Node`, that means that when we call the `CurriedFilterNode` function, we are not receiving a result, but we are receiving another function that needs to be called at a later stage to actually compute the filtered list of values:

```markup
pkg.CurriedFilterNode(func(i int) bool { return i%2 == 0 }}
```

This is the crucial part of making our pipeline builder work. If we look at `ChainPipes` again, the main loop is calling the nodes (functions) that were supplied to it with the channel as input and reassigning the output to the same channel that was used as input:

```markup
        for _, node := range nodes {
                in = node(in)
        }
```

We could go one step further and also abstract away the generator from the `ChainPipes` function:

```markup
func ChainPipes[A any](gn GeneratorNode[A], nodes
  ...Node[A]) []A {
        in := gn()
        for _, node := range nodes {
                in = node(in)
        }
        return Collector(in)
}
```

With this change in place, it does imply that when calling the function, we need another curried function to supply the generator. This can be done in-line, but for clarity, the following example is a separate function existing at the package level. In this case, we will use the `Cat` function that we introduced earlier and return the curried version:

```markup
func CurriedCat(filepath string) func() <-chan string {
        return func() <-chan string {
                out := make(chan string)
                f, err := ioutil.ReadFile(filepath)
                if err != nil {
                        panic(err)
                }
                go func() {
                        lines := strings.Split(string(f),
                            "\n")
                        for _, line := range lines {
                                out <- line
                        }
                        close(out)
                }()
                return out
        }
}
```

Once again, this curried version of the function operates in the same way as the non-curried version. However, through currying, we can make it adhere to the type signature indicated by the `ChainPipes` function. We can now pass both the generator as well as the nodes to this function:

```markup
func main() {
        out := ChainPipes[string](CurriedCat("./main.go"),
                CurriedFilterNode(func(s string) bool {
                    return strings.Contains(s, "func") }),
                CurriedMapNode(func(i string) string {
                    return "line contains func: " + i }))
        fmt.Printf("%v\n", out2)
}
```

Notice that in the preceding example, we did have to give a type hint to `ChainPipes` to indicate the resulting type of the `CurriedCat` function. What we saw in the preceding section is that by using channels, the Go type system, higher-order functions, and more specifically, function currying, we can build programs by chaining together functions in the right way. Using this method of function composition, it’s also easier to refactor our application. If we want to apply a map before filtering, we just need to change the order in which the node is passed to `ChainPipes`.

Just Imagine

# Summary

In this chapter, we took a look at how Go’s concurrency model can be used when writing code in the functional paradigm. We started the chapter with a brief discussion on the difference between concurrency, parallelism, and distributed computing to delineate exactly what concurrency is.

Once we established that concurrency is the ability to do multiple tasks at once (although not necessarily simultaneously), we looked at how we can refactor the functions from [_Chapter 6_](https://subscription.imaginedevops.io/book/programming/9781801811163/2B18771_06.xhtml#_idTextAnchor101) into a concurrent implementation, leveraging channels and goroutines. We concluded this chapter by looking at pipelines, a way to create programs by composing functions together and orchestrating the flow of data with the use of channels. We also looked at how we can create a higher-order function to compose functions (`ChainPipes`) and have observed how, through the use of function currying, we can create functions that adhere to our type system without giving up type safety.

In the next and final chapter, we will take a look at programming libraries that we can leverage to create Go programs, following some of the functional programming principles that we explored in this book.